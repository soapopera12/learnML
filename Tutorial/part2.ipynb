{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29b50feb-4bb7-4477-b7aa-0aa72948ef80",
   "metadata": {},
   "source": [
    "# Supervised Learning Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d297d6d-55f0-4c35-877c-f4414e3b70f2",
   "metadata": {},
   "source": [
    "## Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f421fa-14c4-49fd-a0e8-696b92b195f3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Linear regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17f66a8-8e3a-4fa5-8239-88e6d9a3bb8b",
   "metadata": {},
   "source": [
    "objective → find the hypothesis function → get target value\n",
    "\n",
    "optimize → hypothesis function based on →  simple weighted sum of input features\n",
    "\n",
    "loss/Error  → should be used for optimization → MSE, MAE etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1814e66-699c-47f5-a13d-3541604848ad",
   "metadata": {},
   "source": [
    "Equation for linear regression → $y = \\beta_0 + \\beta_1 x + \\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72668fe-dbaa-434e-a462-dc22d8f93f9b",
   "metadata": {},
   "source": [
    "#### How to get values of the coefficients (weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae1b390-db91-49cb-8f55-85b37190c798",
   "metadata": {},
   "source": [
    "##### **Normal equation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b6b59a-a650-47ec-b7ae-b96d2d45c44f",
   "metadata": {},
   "source": [
    "$w = (X^TX)^{-1}X^Ty$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65c8090-9034-4060-9436-11e1b08af5a3",
   "metadata": {},
   "source": [
    "##### **Singular Value Decomposition (SVD)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c95194-7c85-4326-a301-ee8742e40169",
   "metadata": {},
   "source": [
    "as computation complexity of $(X^TX)^{-1}$ is huge and inverse for some matrix cannot be found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3222fd39-507f-4f28-a4ca-441662b28abe",
   "metadata": {},
   "source": [
    "##### **Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8759af70-4d2d-4a5b-8008-e7eb21b6b5ab",
   "metadata": {},
   "source": [
    "Types of gradient descent\n",
    "\n",
    "1. Batch gradient Descent → for full dataset\n",
    "2. Stochastic gradient descent → for single random instance\n",
    "3. mini-batch gradient descent → for small parts of dataset\n",
    "\n",
    "Steps in gradient descent\n",
    "\n",
    "1. start by filling θ with random values (this is called random initialization).\n",
    "2. Compute the Cost Function (MSE) → depending on the type this can be for a single instance, a batch or the whole training set\n",
    "3. Compute the Gradient (derivative of the cost function that is being used).\n",
    "4. Update the Parameters.\n",
    "5. Repeat Until Convergence.\n",
    "\n",
    "The derivative of the cost function with respect to each parameter tells us how the cost function changes as we change each parameter. Specifically, it indicates the direction in which we should adjust the parameters to decrease the cost function.\n",
    "\n",
    "Here is an example of SGD\n",
    "\n",
    "$y = w_1x_1 + w_2x_2 + b$\n",
    "\n",
    "$L = \\frac{1}{2}(y_{true} - y_{pred})^2$\n",
    "\n",
    "let’s assume we have training example → $(x_1, x_2, y_{true}) = (1, 2, 3)$\n",
    "\n",
    "Initialize the weights $w_1 = 0.1$, $w_2 = 0.2$ and $b = 0.3$\n",
    "\n",
    "so $y_{pred} = w_1x_1 + w_2x_2 + b$\n",
    " = 0.1 * 1 + 0.2 * 2 + 0.3 = 0.8\n",
    "\n",
    "therefore L = $\\frac{1}{2}(3 - 0.8)^2 = 2.42$\n",
    "\n",
    "Computing the gradients \n",
    "\n",
    "$\\frac{\\delta L}{\\delta w_1} = \\frac{\\delta}{\\delta w_1} \\frac{1}{2}(y_{true} - y_{pred})^2$\n",
    "\n",
    "$=  2 . \\frac{1}{2}.(y_{true} - y_{pred}) ( \\frac{\\delta y_{pred}}{\\delta w_1} + \\frac{\\delta y_{true}}{\\delta w_1})$\n",
    "\n",
    " $= (y_{true} - y_{pred}).\\frac{\\delta y_{pred}}{\\delta w_1}$ \n",
    "\n",
    "$= (y_{true} - y_{pred})\\frac{\\delta}{\\delta w_1} (w_1x_1 + w_2x_2 + b)$\n",
    "\n",
    " $= (y_{true} - y_{pred}). x_1$\n",
    "\n",
    "So it is essentially multiplication of the loss with respect to the weight in the updation step, this applies for each variable\n",
    "\n",
    "Therefore \n",
    "\n",
    "$\\frac{\\delta L}{\\delta w_1} = -2.2$     \n",
    "\n",
    "$\\frac{\\delta L}{\\delta w_2} = -4.4$ \n",
    "\n",
    "Updating the weights\n",
    "\n",
    "if $\\alpha = 0.01$\n",
    "\n",
    "$w_1 = w_1 - \\alpha \\frac{\\delta L}{\\delta w_1} = 0.122$\n",
    "\n",
    "$w_2 = w_2 - \\alpha \\frac{\\delta L}{\\delta w_2} = 0.244$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a536dd-1dce-42a9-bb0f-b3ea051ac26e",
   "metadata": {},
   "source": [
    "#### **Regularization of linear models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25cde01-6d96-4164-bf74-a6e45ab23a4a",
   "metadata": {},
   "source": [
    "reduces overfitting → that can be caused by outliers\n",
    "\n",
    "Must scale the data before regularization as it is sensitive to scale of input features.\n",
    "\n",
    "Basically the value of MSE increase by adding a certain regularization parameter to the MSE. This will also increase the value of derivation of MSE, which should reduce the weights of the parameters by a slightly larger value during the update parameters step when compared to when we were not using regularization. Thereby keeping the weights as small as possible.\n",
    "\n",
    "How does L1 and L2 reduce the different weights present in the model?\n",
    "\n",
    "https://www.quora.com/How-does-the-L1-regularization-method-help-in-feature-selection\n",
    "\n",
    "To understand how L1 helps in feature selection, you should consider it in comparison with L2.\n",
    "\n",
    "- L1’s penalty: $\\sum w_i$ → derivation of this → 1\n",
    "- L2’s penalty: $\\sum w_i^2$ → derivation of this → $2 * w_i$  - 1\n",
    "\n",
    "Observation:\n",
    " L1 penalizes weights equally regardless of the magnitude of those weights. L2 penalizes bigger weights more than smaller weights.\n",
    "\n",
    "For example, suppose $w_3 = 100$ and  $w_4=10$\n",
    "\n",
    "- By reducing $w_3$ by 1, L1’s penalty is reduced by 1. By reducing w4 by 1, L1’s penalty is also reduced by 1.\n",
    "- By reducing $w_3$ by 1, L2’s penalty is reduced by 199. By reducing $w_4$ by 1, L2’s penalty is reduced by only 19. Thus, L2 tends to prefer reducing w3 over w4.\n",
    "\n",
    "In general, when a weight $w_i$ has already been small in magnitude, L2 does not care to reduce it to zero, L2 would rather reduce big weights than eliminate small weights to 0. The result is that the weights are reduced, but almost never reduced to 0, i.e. almost never be completely eliminated, meaning no feature selection. On  the other hand, L1 cares about reducing big weights and small weights equally. For L1, the less informative features get reduced. Some features may get completely eliminated by L1, thus we have feature selection.\n",
    "\n",
    "Tl;DR\n",
    "\n",
    "Increasing the MSE will also increase the weights to be decreased at each instance of gradient descent.\n",
    "\n",
    "**Ridge regression (L2)**\n",
    "\n",
    "$L = \\frac{1}{m} \\sum_{i=1}^m(y_{true} - y_{pred}) + \\lambda \\sum_{j=1}^n (w_j^2)$\n",
    "\n",
    "**Lasso regression (L1)**\n",
    "\n",
    "$L = \\frac{1}{m} \\sum_{i=1}^m(y_{true} - y_{pred}) + \\lambda \\sum_{j=1}^n |w_j|$\n",
    "\n",
    "**Elastic Net regression (Both)**\n",
    "\n",
    "weighted sum of L1 and L2 regularization\n",
    "\n",
    "$L = \\frac{1}{m} \\sum_{i=1}^m(y_{true} - y_{pred}) + \\lambda_1 \\sum_{j=1}^n (w_j^2) + \\lambda_2 \\sum_{j=1}^n |w_j|$\n",
    "\n",
    "**Early stopping**\n",
    "\n",
    "    storing the weights for the lowest RMSE on the validation data set.\n",
    "    \n",
    "    This is also a way of regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc9eda2-8346-4b31-9503-0cfc11e13859",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Polynomial regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b61eb42-6f70-42f2-aed4-ed72f3387d02",
   "metadata": {},
   "source": [
    "What if data is more complex than a straight line?\n",
    "\n",
    "add power of each feature as a new feature → using a linear model to fit non-linear data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ce0c7c-4fd8-464a-9f9b-5d048518ef48",
   "metadata": {},
   "source": [
    "## Classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d103910-d510-4fba-ad6d-034716c061d3",
   "metadata": {},
   "source": [
    "Linear classification\n",
    "\n",
    "Non-linear classification\n",
    "\n",
    "1. If some dataset is not linearly separable you can always add polynomial features resulting in linearly separable dataset.\n",
    "2. Add similar computed features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20527dfa-ca94-4fdd-883a-a45e00056344",
   "metadata": {},
   "source": [
    "### **Logistic regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5e153a-71de-4cb2-90a6-17f02bfa3285",
   "metadata": {},
   "source": [
    "objective → find the hypothesis function  and pass the output of the hypothesis function through a activation function (sigmoid) → get target value\n",
    "\n",
    "optimize → activation functions output of hypothesis function based on →  simple weighted sum of input features\n",
    "\n",
    "loss/Error  → should be used for optimization → Log loss, maximum likelihood etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d0111b-ceec-4f39-95e9-28e17132b11b",
   "metadata": {},
   "source": [
    "Equation for Logistic regression → $\\hat{y} = \\frac{1}{e^{-(\\beta_0 + \\beta_1 x + \\epsilon)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6bdc0-3308-4a42-ac3a-df62875942ae",
   "metadata": {},
   "source": [
    "How Sigmoid function squeezes the input values between 0 and 1?\n",
    "\n",
    "$Sigmoid(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "value of e → euler’s constant → 2.7182818\n",
    "\n",
    "1. x can be a positive or a negative number\n",
    "2. if x is very large $e^{-x}$ becomes very small (close to 0), and 1 + 0 becomes 1 so the reciprocal of 1 is 1.\n",
    "3. If x is very small $e^{-x}$ becomes very large so the sum $1 + e^{-x}$ becomes very large and the reciprocal is close to 0.\n",
    "4. Therefore for a large value of x the function becomes 1 and for negative value the function becomes 0, the function output is 0.5 for value of x around 0.\n",
    "\n",
    "x → -ve → close to 0   |    x → 0 → close to 0.5    |    x → +ve  → close to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df72c4e8-7406-45bb-844f-24fd29ea12e3",
   "metadata": {},
   "source": [
    "**Loss function**\n",
    "\n",
    "$LOSS = -\\frac{1}{N}\\sum_{i=1}^N [y_i log(\\hat{y_i}) + (1 - y_i) log(1 - \\hat{y_i})]$\n",
    "\n",
    "$\\hat{y_i}$ is predicted value (0 or 1).\n",
    "\n",
    "$y_i$ is actual value (0 or 1).\n",
    "\n",
    "If $\\hat{y_i}$  is close to 1 → $log(\\hat{y_i})$ is close to 0  |  If $\\hat{y_i}$  is close to 0 → $log(\\hat{y_i})$ is high -ve number → check plot of $log_{10}x$ on google to understand [only between 0 and 1]\n",
    "\n",
    "If $\\hat{y_i}$  is close to 0 →  $log(1 - \\hat{y_i})$ will be close to 1  |   $\\hat{y_i}$  is close to 1 →  $log(1 - \\hat{y_i})$ will be a high -ve number\n",
    "\n",
    "- If $y_i$ is 1 and $\\hat{y_i}$  is close to 1,  $log(\\hat{y_i})$ is close to 0, resulting in a low loss.\n",
    "- If $y_i$ is 1 and $\\hat{y_i}$  is close to 0,  $log(\\hat{y_i})$ is very negative, resulting in a high loss.\n",
    "- If $y_i$ is 0 and $\\hat{y_i}$  is close to 0,  $log(\\hat{y_i})$ is close to 0, resulting in a low loss.\n",
    "- If $y_i$ is 0 and $\\hat{y_i}$  is close to 1,  $log(\\hat{y_i})$ is very negative, resulting in a high loss.\n",
    "\n",
    "Finally Averaging the loss across the whole dataset gives an estimate of how well is our model doing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d04eb-6240-41f1-8bf1-9093565e3f18",
   "metadata": {},
   "source": [
    "**Evaluation of Logistic regression**\n",
    "\n",
    "True Positive (TP), True Negative (TN), False Positive (FP) and False Negative (FN)\n",
    "\n",
    "Accuracy = $\\frac{TP + TN}{TP+TN+FP+FN}$ (how often are models prediction and correct)\n",
    "\n",
    "Precision = $\\frac{TP}{TP + FP}$ (how many positive predictions are actually correct)  → model’s output positivity\n",
    "\n",
    "Recall = $\\frac{TP}{TP + FN}$ (how many actual positives were predicted correctly)  → actual data positivity\n",
    "\n",
    "F1 score = $2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$ (harmonic mean of precision and recall)\n",
    "\n",
    "AUC-ROC → True positive rate (Recall) vs False positive rate for different threshold (default = 0.5)\n",
    "\n",
    "AUC-PR → Precision (y-axis) vs recall (x-axis) curve (should be close to 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b536d2-1eb7-4317-9d2b-965e246b4fdc",
   "metadata": {},
   "source": [
    "### 95% confidence interval for the test set accuracy of a classification model\n",
    "\n",
    "A 95% confidence interval (CI) for a parameter, such as test set accuracy, provides a range within which we can be 95% confident that the true value of the parameter lies.\n",
    "\n",
    "**Step 1: Collect the Predictions and Actual Values**\n",
    "\n",
    "Make predictions on the test set and compare them to the actual labels to obtain the number of correct and incorrect predictions.\n",
    "\n",
    "**Step 2: Calculate the Test Set Accuracy**\n",
    "\n",
    "$Accuracy = \\frac{Number \\ of \\ Correct \\ Predictions}{Total \\ number \\ of \\  predictions}$\n",
    "\n",
    "**Step 3: Calculate the Standard Error of the Accuracy**\n",
    "\n",
    "$SE = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}$\n",
    "\n",
    "here $\\hat{p}$ is the accuracy\n",
    "\n",
    "and n is the number of predictions\n",
    "\n",
    "**Step 4: Calculate the Confidence Interval**\n",
    "\n",
    "$CI = \\hat{p} \\pm z.SE$\n",
    "\n",
    "z → value for desired confidence interval → for 95% → z $\\approx$ 1.96"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62610c3a-23dc-483d-8081-79a311d19084",
   "metadata": {},
   "source": [
    "#### **Softmax regression**\n",
    "\n",
    "support multiple classes\n",
    "\n",
    "lets say there are k classes instead of 2 (binomial).\n",
    "\n",
    "$\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^Ke^z_j}$ \n",
    "\n",
    "$z_i$ =Wx + b\n",
    "\n",
    "So here instead of single set of weights and bias we have same number of set of weights and bias as the number of classes\n",
    "\n",
    "so for 3 classes →       $z_0 = W_0X +b_0$           |         $z_1 = W_1X + b_1$            |     $z_2 = W_2X + b_2$            and so on if more classes are there\n",
    "\n",
    "here instead of sigmoid function we use the softmax function as the activation function\n",
    "\n",
    "**Loss Function**\n",
    "\n",
    "$LOSS = - log(P(y|X)) = -log(\\frac{e^{z_y}}{\\sum_{j=1}^Ke^z_j})$\n",
    "\n",
    "This is called categorical cross entropy loss.\n",
    "\n",
    "How it works?\n",
    "\n",
    "1. Compute logits\n",
    "    \n",
    "    z =Wx + b\n",
    "    \n",
    "    z = [$z_0, z_1, z_2$]  for 3 classes are computed\n",
    "    \n",
    "2. Applying softmax\n",
    "    \n",
    "    Lets say  \n",
    "    \n",
    "    P( y=0 ∣ x) = $\\frac{e^{z_{0}}}{e^{z_{0}} + e^{z_{1}} + e^{z_{2}}}$ \n",
    "    \n",
    "    P(y=1 ∣ x)= $\\frac{e^{z_{1}}}{e^{z_{0}} + e^{z_{1}} + e^{z_{2}}}$ \n",
    "    \n",
    "    P(y=2 ∣ x)= $\\frac{e^{z_{2}}}{e^{z_{0}} + e^{z_{1}} + e^{z_{2}}}$ \n",
    "    \n",
    "3. Extract the Probability for the True Class\n",
    "    \n",
    "    say values of $e^{z_{2}} = 2.0, e^{z_{2}} = 1.0  \\ and \\ e^{z_{2}} = 0.1$\n",
    "    \n",
    "    P( y=0 ∣ x) = $\\frac{e^{z_{0}}}{e^{z_{0}} + e^{z_{1}} + e^{z_{2}}}$ =  $\\frac{e^{2.0}}{e^{2.0}+ e^{1.0}+ e^{0.1}}$ = 0.59\n",
    "    \n",
    "    P(y=1 ∣ x)= $\\frac{e^{z_{1}}}{e^{z_{0}} + e^{z_{1}} + e^{z_{2}}}$ = $\\frac{e^{1.0}}{e^{2.0}+ e^{1.0}+ e^{0.1}}$ = 0.242\n",
    "    \n",
    "    P(y=2 ∣ x)= $\\frac{e^{z_{2}}}{e^{z_{0}} + e^{z_{1}} + e^{z_{2}}}$ =$\\frac{e^{0.1}}{e^{2.0}+ e^{1.0}+ e^{0.1}}$ = 0.009\n",
    "    \n",
    "    Remember    →    P(y=0 | x) + P(y=1 | x) + P(y=2 | x) = 1\n",
    "    \n",
    "4. Compute the Cross-Entropy Loss for the Single Example\n",
    "    \n",
    "    This is only done for the correct label of y, here y should be 1\n",
    "    \n",
    "    therefore $LOSS = -log(P(y = 1|x)) = -log(0.242) = -(-1.418) = 1.418$\n",
    "    \n",
    "\n",
    "Total Loss = $\\frac{1}{N} \\sum_{i=1}^N -log(P(y^i | x^i))$    So we only calculate loss for that target y value only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50c2c64-3e48-418b-8cd0-5753387e4669",
   "metadata": {},
   "source": [
    "#### Support Vector Machine (SVM)\n",
    "\n",
    "widest possible street between classes - basically the best hyperplane that is being used to separate the instances of the two classes (line → plane → hyperplane)\n",
    "\n",
    "soft margin classification\n",
    "\n",
    "hard margin classification - all instances must be off the street\n",
    "\n",
    "linear SVM\n",
    "\n",
    "easy to find the line, plane or hyperplane\n",
    "\n",
    "Large margin classification\n",
    "\n",
    "support vectors are instances at the edge of the street\n",
    "\n",
    "Non-linear SVM\n",
    "\n",
    "Not easy to find the line, plane or hyperplane so we use the kernel trick\n",
    "\n",
    "polynomial kernel\n",
    "\n",
    "kernel trick - without adding higher degree features get the same results\n",
    "\n",
    "Similarity Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c89ddd1-53d3-4eb1-acbe-97ce7154c2e3",
   "metadata": {},
   "source": [
    "#### Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4670ed14-3c3e-4751-a4e3-1400712605f4",
   "metadata": {},
   "source": [
    "objective → At each node find a feature that best separates the dataset (most homogeneously)\n",
    "\n",
    "optimize → least gini impurity or entropy at each node for the selected feature in case of classification or least MSE in case of linear regression\n",
    "\n",
    "loss/Error  → should be used for optimization → Gini impurity, Entropy, MSE etc\n",
    "\n",
    "1. Initialize\n",
    "    \n",
    "    Start with the entire dataset at the root of the tree.\n",
    "    \n",
    "2. Splitting criteria\n",
    "    \n",
    "    For each node, evaluate all possible splits for each feature. For a numeric feature, this involves evaluating splits at every distinct value. For a categorical feature, evaluate splits based on subsets of categories.\n",
    "    \n",
    "3. Calculate Impurity/Variance\n",
    "    \n",
    "    Calculate the Gini impurity for each possible split:\n",
    "    \n",
    "    Gini Impurity = $1 - \\sum_{i=1}^{C}p_i^2$    → for classification\n",
    "    \n",
    "    each node has a gini (impurity means how many instances don’t follow a particular nodes rule)\n",
    "    \n",
    "    where $p_i$  is the probability of a sample belonging to class i and C is the total number of classes.\n",
    "    \n",
    "    Entropy = $-\\sum_{i=1}^{C}p_ilog_2(p_i)$    → for classification\n",
    "    \n",
    "     where $p_i$ is the probability of a sample belonging to class i.\n",
    "    \n",
    "4. Select the Best Split\n",
    "    \n",
    "    Choose the split that results in the lowest weighted impurity (for classification) \n",
    "    \n",
    "    Weighted Impurity = $\\frac{N_{left}}{N}Impurity+ \\frac{N_{right}}{N}Impurity$\n",
    "    \n",
    "    Where N is the total number of samples in the node, $N_{left} \\ and \\ N_{right}$  are the number of samples in the left and right child nodes, respectively.\n",
    "    \n",
    "5. Split the Node\n",
    "    \n",
    "    Create two child nodes based on the selected split.\n",
    "    \n",
    "    Assign the data points to the appropriate child node based on the split criterion.\n",
    "    \n",
    "6. Stopping Criteria\n",
    "    \n",
    "    Maximum tree depth is reached.\n",
    "    \n",
    "7. Repeat Recursively\n",
    "    \n",
    "    Apply the same process recursively to each child node, treating each child node as a new parent node.\n",
    "    \n",
    "\n",
    "Branches represent decision \n",
    "\n",
    "leaves represent final output or classification outcome\n",
    "\n",
    "feature scaling is not required mostly and can lead to decrease in performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a2928b-fdd1-4960-8eb7-43f64d577e17",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n",
    "\n",
    "Bayes' Theorem describes the probability of an event based on prior knowledge of conditions that might be related to the event.\n",
    "\n",
    "$P(A|B) = \\frac{P(B|A).P(A)}{P(B)}$\n",
    "In a classification context, we aim to find the class C that maximizes P(C∣X), where X is the set of features. \n",
    "\n",
    "$P(C|X) = \\frac{P(X|C).P(C)}{P(X)}$\n",
    "\n",
    "this can be simplified to \n",
    "\n",
    "$P(C|X) = {P(X|C).P(C)}$\n",
    "\n",
    "The naive assumption is that the feature is conditionally independent to the class\n",
    "\n",
    "$P(X|C) = P(x_1,x_2,...x_n|C) = P(x_1|C).P(x_2|C)...P(x_n|C)$\n",
    "\n",
    "Example\n",
    "\n",
    "| Email | Contains \"Buy\" | Contains \"Cheap\" | Contains \"Click\" | Contains \"Limited\" | Class |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "\n",
    "| 1 | Yes | No | Yes | No | Spam |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "\n",
    "| 2 | No | No | No | Yes | Not Spam |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "\n",
    "| 3 | Yes | Yes | Yes | No | Spam |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "\n",
    "| 4 | No | Yes | No | No | Not Spam |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "\n",
    "| 5 | Yes | Yes | Yes | Yes | Spam |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "1. Calculating prior probabilities\n",
    "\n",
    "P(spam) = 0.6\n",
    "\n",
    "P(not spam) = 0.4\n",
    "\n",
    "1. Calculate likelihoods\n",
    "    \n",
    "    Calculate $P(x_i∣Spam)$ and $P(x_i|Not  Spam)$ for each feature.\n",
    "    \n",
    "    | Feature | P(Yes|Spam) | P(No|Spam) | P(Yes|Not Spam) | P(No|Not Spam) |\n",
    "    |-----------------|-------------|-----------|-----------------|----------------|\n",
    "    | Contains \"Buy\" | 3/3 = 1.0 | 0/3 = 0 | 0/2 = 0 | 2/2 = 1.0 |\n",
    "    | Contains \"Cheap\"| 2/3 = 0.67 | 1/3 = 0.33| 1/2 = 0.5 | 1/2 = 0.5 |\n",
    "    | Contains \"Click\"| 2/3 = 0.67 | 1/3 = 0.33| 0/2 = 0 | 2/2 = 1.0 |\n",
    "    | Contains \"Limited\"| 1/3 = 0.33| 2/3 = 0.67| 1/2 = 0.5 | 1/2 = 0.5 |\n",
    "    \n",
    "2. Classify new mails\n",
    "    \n",
    "    We can use this mail to now check the $P(C|X) = {P(X|C).P(C)}$ probability of a class occurring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e011a13-cf69-4d92-b629-ede607a5c134",
   "metadata": {},
   "source": [
    "#### Ensemble models\n",
    "\n",
    "**This is very important**\n",
    "\n",
    "Wisdom of the crowd\n",
    "\n",
    "Aggregating the predictions of a group of predictors is called an ensemble model\n",
    "\n",
    "Ensemble models\n",
    "\n",
    "1. **Bagging (Bootstrap aggregating)**\n",
    "    \n",
    "    same training model but different random subsets of training data for each predictor\n",
    "    \n",
    "    **sampling with replacement** → after a datapoint is chosen to be a part of the sample it is replaced back into the dataset to be picked again in subsequent draws.\n",
    "    \n",
    "    **Out-of-bag (OOB) evaluation**\n",
    "    \n",
    "    It can be shown mathematically that 67% of the training instances are used by bagging (with replacement) and rest 33% are not used (for a single classfier but i can be used by other classifiers in the ensemble).\n",
    "    \n",
    "    This 33% can be used as testing data. With enough estimators the whole training data can be then used as testing data also\n",
    "    \n",
    "    one example is random forest, Xtra trees\n",
    "    \n",
    "2. **Pasting**\n",
    "    \n",
    "    same training model but different random subsets of training data for each predictor\n",
    "    \n",
    "    **sampling without replacement** → after a datapoint is chosen to be a part of the sample it is cannot be replaced back into the dataset to be picked again in subsequent draws.\n",
    "    \n",
    "    example:- same like bagging example random forest but the data is without replacement\n",
    "    \n",
    "3. **Boosting**\n",
    "    \n",
    "    Boosting focuses on training models sequentially, where each new model attempts to correct the errors of its predecessor. This technique aims to reduce bias and improve model accuracy.\n",
    "    \n",
    "    Processz`\n",
    "    \n",
    "    1. Train an initial base model on the entire training data.\n",
    "    2. Evaluate the model and increase the weight of incorrectly predicted instances.\n",
    "    3. Train a new model using the updated weights.\n",
    "    4. Repeat the process, combining the models in a weighted manner (e.g., weighted majority voting).\n",
    "    \n",
    "    Types of boosting \n",
    "    \n",
    "    1. Adaboost (Adaptive boosting)\n",
    "        \n",
    "        Adjusts the weights of incorrectly classified instances and train the new models on updated weights.\n",
    "        \n",
    "    2. Gradient boosting\n",
    "        \n",
    "        Fits new model to the residual errors of previous models.\n",
    "        \n",
    "        More flexible than adaboost and can handle non-linear relationships, better for higher dimensional data\n",
    "        \n",
    "    3. XGBoost (Extreme Gradient Boosting)\n",
    "        \n",
    "        An optimized and efficient implementation of gradient boosting. Has parallel processing capabilities.\n",
    "        \n",
    "    4. CatBoost (Categorical Boosting)\n",
    "        \n",
    "        Optimal for categorical data. No need of one-hot-encoding.\n",
    "        \n",
    "    5. LightGBM\n",
    "        \n",
    "        Tree based learning algorithms and one-side sampling technique.\n",
    "        \n",
    "4. **Stacking**\n",
    "    \n",
    "    Stacking involves training multiple different types of models and then using another model to combine their predictions. This technique leverages the strengths of various base models. So the underlying model can be thought of as base models and then we can use a simple Logistic regression model to combine the result of these model to get us an output.\n",
    "    \n",
    "    Process:\n",
    "    \n",
    "    1. Train several different base models on the training data.\n",
    "    2. Use the predictions of these base models as input features for a second-level model (meta-model).\n",
    "    3. Train the meta-model to make the final prediction based on the outputs of the base models.\n",
    "    \n",
    "    examples are LR, Decision tree and SVM for base models and LR for meta models\n",
    "    \n",
    "5. **Voting classifiers (Not a ensemble model but rather deciding how the ensemble models O/Ps are found.**\n",
    "    \n",
    "    Voting ensembles combine the predictions of multiple models by voting, either through majority voting for classification or averaging for regression.\n",
    "    \n",
    "    1. Hard Voting\n",
    "        \n",
    "        Each model votes for a class, and the class with the majority votes is the final prediction.\n",
    "        \n",
    "    2. Soft Voting\n",
    "        \n",
    "        Each model provides a probability for each class, and the class with the highest average probability is the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3780680-c9ed-4a7d-b11e-bc0ad30b187d",
   "metadata": {},
   "source": [
    "#### Knn Algorithm\n",
    "\n",
    "- **Choosing K:**\n",
    "    - Select the number of neighbors (K). This is a crucial hyperparameter that determines the number of nearest neighbors to consider when making a prediction.\n",
    "- **Calculating Distance:**\n",
    "    - For a given input, calculate the distance between this input and all the points in the training dataset. Common distance metrics include Euclidean distance, Manhattan distance, and Minkowski distance.\n",
    "- **Identifying Neighbors:**\n",
    "    - Identify the K closest neighbors to the input based on the calculated distances.\n",
    "- **Making Predictions:**\n",
    "    - **For Classification:**\n",
    "        - The input is assigned to the class most common among its K nearest neighbors (majority voting).\n",
    "    - **For Regression:**\n",
    "        - The input's predicted value is the average (or sometimes weighted average) of the values of its K nearest neighbors.\n",
    "\n",
    "Main disadvantages\n",
    "\n",
    "considered lazy because → does not scale well mainly because it memorizes the entire dataset and performs actions based on the dataset\n",
    "\n",
    " curse of dimensionality\n",
    "\n",
    "prone to overfitting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7db0ec3-13f9-4758-ad86-2a91e0932f7a",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898d3cac-1af4-4116-97d0-4cc520b00d62",
   "metadata": {},
   "source": [
    "high-dimensional datasets are at risk of being very sparse \n",
    "\n",
    "reduce number of features in dataset while preserving as much information as possible\n",
    "\n",
    "Approaches of DR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4db810-ab8f-47f4-bdd9-414309487796",
   "metadata": {},
   "source": [
    "1. **Projection**\n",
    "    \n",
    "    Projection methods reduce the dimensionality of the data by transforming it onto a lower-dimensional subspace.\n",
    "    \n",
    "    **PCA**\n",
    "    \n",
    "    PCA is a popular linear projection method.  \n",
    "    \n",
    "    It identifies the hyperplane that lies closest to the data, and then it projects the data onto it.\n",
    "    \n",
    "    Steps in PCA\n",
    "    \n",
    "    1. Standardize the Data\n",
    "        \n",
    "        Ensure the data has zero mean and unit variance. This is crucial as PCA is affected by the scale of the features.\n",
    "        \n",
    "        $X_{standardized} = \\frac{X-\\mu}{\\sigma}$\n",
    "        \n",
    "        where μ is the mean and σ is the standard deviation of the features.\n",
    "        \n",
    "        Let’s take an example of 3 features\n",
    "        \n",
    "        Example-\n",
    "        \n",
    "        $X_{standardized} = \\begin{bmatrix}\n",
    "        1.2 & -0.9 & 2.1\\\\\n",
    "        0.8 & -1.1 & 2.5\\\\\n",
    "        1.0 & -1.0 & 2.3\n",
    "        \\end{bmatrix}$\n",
    "        \n",
    "        *Standardizing the data ensures that each feature contributes equally to the PCA computation.*\n",
    "        \n",
    "    2. Compute the Covariance Matrix\n",
    "        \n",
    "        The covariance matrix captures the pairwise correlations between features.\n",
    "        \n",
    "        $\\sum = \\frac{1}{n-1}X_{standardized}^TX_{standardized}$\n",
    "        \n",
    "        where n is the number of samples.\n",
    "        \n",
    "        Example-\n",
    "        \n",
    "        $\\sum = \\begin{bmatrix}\n",
    "        0.02 & 0.001 & 0.04\\\\\n",
    "        0.001 & 0.03 & 0.002\\\\\n",
    "        0.03 & 0.002 & 0.006\n",
    "        \\end{bmatrix}$\n",
    "        \n",
    "        *The covariance matrix represents the covariance (joint variability) between pairs of features.*\n",
    "        \n",
    "    3. Compute Eigenvalues and Eigenvectors\n",
    "        \n",
    "        The eigenvalues represent the amount of variance explained by each principal component, and the eigenvectors represent the directions of these components.\n",
    "        \n",
    "        $\\sum v = \\lambda v$\n",
    "        \n",
    "        where λ is an eigenvalue and v is the corresponding eigenvector.\n",
    "        \n",
    "        Example-\n",
    "        \n",
    "        Suppose the eigen values are $\\lambda_1$=0.07, $\\lambda_2$= 0.04 $\\lambda_3$= 0.003.\n",
    "        \n",
    "        The corresponding eigen vectors are $v_1, v_2$ and $v_3$.\n",
    "        \n",
    "        *The eigenvectors of the covariance matrix are the principal components. Eigenvalues correspond to the variance explained by each component.* \n",
    "        \n",
    "        *For instance, the first principal component is the direction that maximizes the variance in the data.*\n",
    "        \n",
    "    4. Sort Eigenvalues and Select Principal Components\n",
    "        \n",
    "        Sort the eigenvalues in descending order and select the top k eigenvectors corresponding to the largest eigenvalues. These eigenvectors form the principal components.\n",
    "        \n",
    "        Example-\n",
    "        \n",
    "        Choose the top two eigenvectors based on the eigenvalues: $v_1$ and $v_2$\n",
    "        \n",
    "    5. Transform the Data\n",
    "        \n",
    "        Project the original data onto the new k-dimensional subspace.\n",
    "        \n",
    "        $X_{PCA} = X_{standardized} W$\n",
    "        \n",
    "        where W is the matrix of the selected eigenvectors.\n",
    "        \n",
    "    \n",
    "    PCA helps in identifying the directions of maximum variance and projecting the data onto these directions to reduce dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e29d361-61cf-4573-a3c1-e0011827c9dd",
   "metadata": {},
   "source": [
    "2. **Manifold learning**\n",
    "    \n",
    "    Manifold learning methods assume that the high-dimensional data lies on a low-dimensional manifold within the higher-dimensional space. \n",
    "    \n",
    "    Popular manifold structure\n",
    "    \n",
    "    1. **t-Distributed Stochastic Neighbor Embedding (t-SNE)**\n",
    "    2. **Isomap**\n",
    "    3. **Locally Linear Embedding (LLE)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cuda118",
   "language": "python",
   "name": "cuda118"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
