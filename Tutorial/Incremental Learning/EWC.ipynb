{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "262549c5-d562-4fce-b812-48491281155c",
   "metadata": {},
   "source": [
    "# Incremental Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a765cc71-56b2-4167-853e-4298c422f6b5",
   "metadata": {},
   "source": [
    "## Elastic Weight Consolidation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae49270-ed0a-4afc-805e-9272599f3807",
   "metadata": {},
   "source": [
    "Denote parameters of layers of a deep neural network (DNN) with θ. Training DNNs generates a mapping between the input distribution space and target distribution space.\n",
    "\n",
    "This is done by finding out an optimum $\\theta$ = $\\theta^*$ which results in the least error in the training objective. It has been shown in earlier works that such a mapping can be obtained \n",
    "\n",
    "with many configurations of $\\theta^*$ like this image.\n",
    "\n",
    "<div>\n",
    "  <img src=\"Images/Untitled.png\" alt=\"Untitled\" style=\"width: 250px; height: 250px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9778a7b4-7faa-42f5-9827-cffa8e59d9a3",
   "metadata": {},
   "source": [
    "This is basically the solution around the most optimum $\\theta$ with acceptable error in the learned mapping.\n",
    "\n",
    "Let’s begin with a simple case of two tasks, task A and task B. To have a configuration of parameters that performs well for both A and B, the network should be able to pick θ \n",
    "\n",
    "from the overlapping region of the individual solution spaces. In the first instance, the network can learn any θ = $\\theta_A$ that performs well for task A. But with the arrival of\n",
    "task B, the network should pick up a θ = $\\theta_{A,B}$. The next question that arrives is how can the network learn the a set of parameters that lies in this overlapping region.\n",
    "\n",
    "To this end, EWC presents a method of selective regularization of parameters θ. After learning A, this regularization method identifies which parameters are important for A, and then penalizes any change made to the network parameters according to their importance while learning B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ac3578-85fc-4c56-87c8-97f3618862d3",
   "metadata": {},
   "source": [
    "<div>\n",
    "  <img src=\"Images/2tasks.png\" alt=\"2tasks\" style=\"width: 250px; height: 250px;\">\n",
    "    <img src=\"Images/4tasks.png\" alt=\"4tasks\" style=\"width: 250px; height: 250px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d4e2f8-8f1d-4df9-bfcc-7f98743c9948",
   "metadata": {},
   "source": [
    "Let’s say the weights to be learned are W = [$w_1, w_2, w_3, ....., w_n]$\n",
    "\n",
    "Now these weights could have a certain type of distribution pattern like\n",
    "\n",
    "1. Normal distribution\n",
    "2. Uniform Distribution\n",
    "3. Laplace distribution\n",
    "4. Dirichlet distribution\n",
    "\n",
    "You can find this using visual or statistical test.\n",
    "\n",
    "So what is the purpose of these distribution in terms of training \n",
    "\n",
    "1. Before training\n",
    "    \n",
    "    Initializing the weights according to a distribution can help in convergence and performance of training\n",
    "    \n",
    "    example: Xavier initialization uses Uniform or normal distribution of initial weights\n",
    "    \n",
    "2. After training\n",
    "    \n",
    "    Analyzing the distribution of weights can help in analyzing properties of model such as sparsity, variance or presence of certain patterns.\n",
    "    \n",
    "\n",
    "To formulate the objective we use a Bayesian approach to estimate the parameters $\\theta$.\n",
    "\n",
    "$P(\\theta|\\sum) = \\frac{P(\\sum|\\theta)P(\\theta)}{P(\\sum)}$                                                                ————————(1)\n",
    "\n",
    "Here $\\sum$ is the data\n",
    "\n",
    "$P(\\theta|\\sum)$ is the posterior\n",
    "\n",
    "$P(\\sum|\\theta)$  is the likelihood\n",
    "\n",
    "$P(\\theta)$ is the prior\n",
    "\n",
    "and we want to learn the posterior PDF $P(\\theta|\\sum)$\n",
    "\n",
    "So maximizing a function is same as maximizing its log \n",
    "\n",
    "log($P(\\theta|\\sum)$) = log($P(\\theta|\\sum)$) + log($P(\\theta)$) - log($P(\\sum)$)             ————————(2)\n",
    "\n",
    "Now to train a Neural Network is to maximize this logarithm\n",
    "\n",
    "$arg \\max\\limits_{\\theta} \\{ l(\\theta) = log(P(\\theta|\\sum)) \\}$\n",
    "\n",
    "We can write (2) as\n",
    "\n",
    "$log(p(\\theta|\\sum)) = log(p(B|A, \\theta) + log(p(\\theta|A)) - log(p(B|A))$   ————————(3)\n",
    "\n",
    "$log(p(\\theta|\\sum)) = log(p(B| \\theta) + log(p(\\theta|A)) - log(p(B))$  ————————(4)\n",
    "\n",
    "But  $log(p(\\theta|A))$ is intractable so they calculated its approimation and the overall loss function becomes\n",
    "\n",
    "$l(\\theta) = l_B(\\theta) - \\frac{\\lambda}{2}(\\theta - \\theta^*_A)^T []_A (\\theta - \\theta^*_A)+\\epsilon'$                                ————————(5)\n",
    "\n",
    "where $[]_A = E [-\\frac{\\delta^2(log(p(\\theta|A)))}{\\delta^2\\theta}|_{\\theta^*_A}]$ = $E[ ((\\frac{\\delta(log(p(\\theta|A)))}{\\delta\\theta})(\\frac{\\delta(log(p(\\theta|A)))}{\\delta\\theta})^T)|_{\\theta^*_A}]$   is the Fisher Information Matrix(FIM)\n",
    "\n",
    "The Fisher Information Matrix (FIM) holds the importance of weights for previous task say A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8a7861-7799-44ce-b90d-fc3fdf32e311",
   "metadata": {},
   "source": [
    "<div>\n",
    "  <img src=\"Images/regularization.png\" alt=\"regularization\" style=\"width: 800px; height: 200px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c754ad6-92d9-4805-9e90-e8cf2c114183",
   "metadata": {},
   "source": [
    "Pytorch Implementation\n",
    "\n",
    "It is pretty straight forward\n",
    "\n",
    "All we have to do is train the model on say task A.\n",
    "\n",
    "But while training on task B we have to use the FIM (precision matrices) for the model based on model trained for task A.\n",
    "\n",
    "For which all i have to do is find the gradient for each parameter based on data for task A and square this value for each data instance\n",
    "\n",
    "Now we have out importance of each parameter array.\n",
    "\n",
    "Now while training for task B all we have to do is add a penalty term which is basically calculating the difference of the current parameters to the old model parameters multiplied by the FIM or precision matrices value.\n",
    "\n",
    "https://github.com/moskomule/ewc.pytorch/tree/master\n",
    "\n",
    "Issues with EWC\n",
    "\n",
    "1. Scalability issues.\n",
    "    \n",
    "    Computation of FIM is computationally expensive and memory-intensive.\n",
    "    \n",
    "2. Assumption of independent matrix\n",
    "    \n",
    "    EWC assumes that the parameters are independent, meaning that it only considers the diagonal elements of the Fisher Information Matrix. This assumption ignores the potential correlations between different parameters, which can lead to suboptimal performance in preserving knowledge.\n",
    "    \n",
    "3. Task similarity and importance weighting\n",
    "    \n",
    "    The method relies on the Fisher Information Matrix to determine the importance of weights. However, this matrix might not capture the true importance of weights across very different tasks. If tasks are significantly different, the Fisher Information Matrix from previous tasks might not be a good indicator of weight importance for future tasks.\n",
    "    \n",
    "4. Storage of Past Information\n",
    "    \n",
    "    While EWC reduces the need to store entire datasets of past tasks, it still requires storing the learned parameters and their corresponding Fisher Information Matrices. For a large number of tasks, this can lead to significant storage requirements, which can become impractical over time.\n",
    "    \n",
    "5. Diminishing Effectiveness with Many Tasks\n",
    "    \n",
    "    As the number of tasks increases, the regularization terms from multiple tasks accumulate. This can lead to a scenario where the model becomes overly constrained, reducing its capacity to learn new tasks effectively. This phenomenon is often referred to as \"**regularization collapse**.\"\n",
    "    \n",
    "6. Empirical Performance\n",
    "    \n",
    "    In practice, the performance of EWC can vary significantly depending on the specific tasks and datasets. Some studies have shown that EWC can be less effective compared to other continual learning methods, especially in scenarios where tasks are very diverse or when the number of tasks is large.\n",
    "    \n",
    "7. Complex Hyperparameter Tuning\n",
    "    \n",
    "    EWC introduces additional hyperparameters, such as the regularization strength λ\\lambdaλ. Tuning these hyperparameters can be complex and time-consuming, and the optimal values can vary widely depending on the tasks and datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cuda118",
   "language": "python",
   "name": "cuda118"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
