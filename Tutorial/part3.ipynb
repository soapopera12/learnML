{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceb71743-70fe-4132-92f0-8bec1e993d5c",
   "metadata": {},
   "source": [
    "## Unsupervised Learning Techniques\n",
    "\n",
    "---\n",
    "\n",
    " The vast majority of the available data is unlabeled\n",
    "\n",
    "We have the input features X, but we do not have the labels y\n",
    "\n",
    "Unsupervised learning tasks → clustering, anomaly detection and Density estimation\n",
    "\n",
    "---\n",
    "\n",
    "### k-means\n",
    "\n",
    "given all the instance labels, locate each cluster’s centroid by computing the mean of the instances in that cluster - both centroids and labels are not given\n",
    "\n",
    "Process\n",
    "\n",
    "1. place the centroids randomly (randomly select k instances from the training set).\n",
    "2. Assign each data point to the nearest centroid creating k clusters (euclidean distance).\n",
    "3. Recalculate the mean of all the data points for each cluster and assign the new point as the centroid. \n",
    "4. Repeat 2,3 and 4 till the centroids stop moving.\n",
    "\n",
    "example of finding the mean of the clusters\n",
    "\n",
    "lets say we have these data points \n",
    "\n",
    "`Points: (1, 2), (2, 3), (3, 4), (8, 8), (9, 10), (10, 11)`\n",
    "\n",
    "and we want to make k = 2 clusters\n",
    "\n",
    "assume intial centroids are\n",
    "\n",
    "Centroid 1: (1, 2)\n",
    "Centroid 2: (10, 11)\n",
    "\n",
    "- Points (1, 2), (2, 3), (3, 4) are closer to Centroid 1 → cluster 1\n",
    "- Points (8, 8), (9, 10), (10, 11) are closer to Centroid 2 → cluster 2\n",
    "\n",
    "Recalculate Centroids:\n",
    "\n",
    "New Centroid 1 = Mean((1, 2), (2, 3), (3, 4))\n",
    "= ((1+2+3)/3, (2+3+4)/3)\n",
    "= (2, 3)\n",
    "\n",
    "New Centroid 2 = Mean((8, 8), (9, 10), (10, 11))\n",
    "= ((8+9+10)/3, (8+10+11)/3)\n",
    "= (9, 9.67)\n",
    "\n",
    "Clustering types\n",
    "\n",
    "Hard clustering \n",
    "\n",
    "directly assign the cluster for a node\n",
    "\n",
    "Soft clustering\n",
    "\n",
    "score per cluster\n",
    "\n",
    "Centroid initialization methods\n",
    "\n",
    "Setting the init hyperparameter (if you have an idea of where the centroid should be)\n",
    "\n",
    "Run algorithm multiple times (with different random initialization for the centroid)\n",
    "\n",
    "Performance metric \n",
    "\n",
    "model’s inertia → sum of the squared distances between the instances and their closest centroids.\n",
    "\n",
    "---\n",
    "\n",
    "### k-means++\n",
    "\n",
    "Smarter initialization step that tends to select centroids that are distant from one another\n",
    "\n",
    "1. Randomly select the first centroid, say $\\mu_1$.\n",
    "2.  Calculate the distance of all points from  $\\mu_1$, then select the second centroid  $\\mu_2$ with a probability proportional to sum of the squared distances.\n",
    "    \n",
    "    Let’s say the distance to the all the points from the current centroid is as follows\n",
    "    \n",
    "    - $D(x_1)$ = 1\n",
    "    - $D(x_2)$ = 2\n",
    "    - $D(x_3)$ = 3\n",
    "    - $D(x_4)$ = 4\n",
    "    \n",
    "    Squaring these distances, we get:\n",
    "    \n",
    "    - $D(x_1)^2$ = 1\n",
    "    - $D(x_2)^2$ = 4\n",
    "    - $D(x_3)^2$ = 9\n",
    "    - $D(x_4)^2$ =16\n",
    "    \n",
    "    The sum of the squared distances is 1+4+9+16=301+4+9+16=30. The probabilities for each point being selected as the next centroid are:\n",
    "    \n",
    "    - $P(x_1)$ = 1/30 ≈ 0.033\n",
    "    - $P(x_2)$ = 4/30 ≈ 0.133\n",
    "    - $P(x_3)$ = 9/30 = 0.3\n",
    "    - $P(x_4)$ = 16/30 ≈ 0.533\n",
    "    \n",
    "    So basically we do not always use the farthest point but as the distance increases the probability increases too .\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "### Accelerated and mini-batch k-means\n",
    "\n",
    "1. **Accelerated k-Means**\n",
    "\n",
    "Accelerated k-Means algorithms aim to speed up the standard k-means clustering process, which can be computationally intensive due to the need to repeatedly compute distances between data points and cluster centroids.\n",
    "\n",
    "1. Elkan's Algorithm\n",
    "    \n",
    "    Elkan's algorithm speeds up k-means by reducing the number of distance calculations needed during each iteration. \n",
    "    \n",
    "    1. **Initialization**: Start with initial centroids, like in standard k-means.\n",
    "    2. **Distance Bounds**: Maintain upper and lower bounds for the distances between each point and the centroids.\n",
    "    3. **Update Centroids**: After assigning points to the nearest centroids, update the centroids.\n",
    "    4. **Bounds Update**: Update the bounds based on the new centroids.\n",
    "    5. **Pruning**: Use the bounds to skip distance calculations for points that are unlikely to change their cluster assignments.\n",
    "2. k-d Tree and Ball Tree Methods\n",
    "    1. k-d Tree → A binary tree where each node represents a splitting hyperplane dividing the space into two subspaces.\n",
    "    2. ball tree → A hierarchical data structure where data points are encapsulated in hyperspheres (balls).\n",
    "\n",
    "1. **Mini-batch k-means**\n",
    "    \n",
    "    Mini-batch k-means is a variant of the k-means algorithm that reduces the computational cost by using small, random samples (mini-batches) of the dataset in each iteration rather than the entire dataset.\n",
    "    \n",
    "    Process\n",
    "    \n",
    "    1. **Initialization**: Start with initial centroids, just like in standard k-means.\n",
    "    2. **Mini-Batch Selection**: Randomly select a mini-batch of data points from the dataset.\n",
    "    3. **Cluster Assignment**: Assign each point in the mini-batch to the nearest centroid.\n",
    "    4. **Centroid Update**: Update the centroids based on the mini-batch. The update rule is typically a weighted average to account for the small size of the mini-batch.\n",
    "    5. **Repeat**: Iterate the mini-batch selection, cluster assignment, and centroid update steps until convergence or a maximum number of iterations is reached.\n",
    "\n",
    "Finding the optimal number of clusters\n",
    "\n",
    "1. Inertia is not a good performance metric when trying to choose k because it keeps getting lower as we increase k.\n",
    "    \n",
    "    $Inertia = \\sum_{i=1}^n \\sum_{j=1}^k 1(c_i=j) || x_i - \\mu_j||^2$\n",
    "    \n",
    "    - n is the number of data points.\n",
    "    - k is the number of clusters.\n",
    "    - $x_i$ is the i-th data point.\n",
    "    - $\\mu_j$ is the centroid of the j-th cluster.\n",
    "    - 1($c_i$ = j) is an indicator function that is 1 if the data point i belongs to cluster j, and 0 otherwise.\n",
    "    - $||x_i -\\mu_j ||^2$ is the squared Euclidean distance between the data point and the cluster centroid.\n",
    "2. Silhouette score\n",
    "    \n",
    "    The Silhouette score is a metric used to evaluate the quality of a clustering algorithm. It measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation).\n",
    "    \n",
    "    For each data point i, the Silhouette score s(i) is calculated using the following steps\n",
    "    \n",
    "    1. Calculate the mean intra-cluster distance (a(i))\n",
    "        \n",
    "        This is the average distance between the data point i and all other points in the same cluster.\n",
    "        \n",
    "        $a(i) = \\frac{1}{|C_i|-1}\\sum_{j \\in C, j\\neq i} d(i,j)$\n",
    "        \n",
    "        where $c_i$ is the cluster containing i, and d(i,j) is the distance between points i and j.\n",
    "        \n",
    "    2. Calculate the mean nearest-cluster distance (b(i))\n",
    "        \n",
    "        This is the average distance between the data point i and all points in the nearest cluster that is not $C_i$\n",
    "        \n",
    "        $b(i) = min_{C_k \\neq C_i} \\frac{1}{|C_k|}\\sum_{j \\in C_k} d(i,j)$\n",
    "        \n",
    "        where $C_k$  is any cluster that is not $C_i$\n",
    "        \n",
    "    3. Calculate the Silhouette score (s(i))\n",
    "        \n",
    "        $s(i) = \\frac{b(i) - a(i)}{max(a(i),b(i))}$\n",
    "        \n",
    "        The Silhouette score ranges from -1 to 1:\n",
    "        \n",
    "        1. s(i) close to 1 indicates that the data point is well-clustered and appropriately assigned.\n",
    "        2. s(i) close to 0 indicates that the data point is on or very close to the decision boundary between two neighboring clusters.\n",
    "        3. s(i) close to -1 indicates that the data point might have been assigned to the wrong cluster.\n",
    "\n",
    "---\n",
    "\n",
    "### DBSCAN\n",
    "\n",
    "density-based spatial clustering of applications with noise (DBSCAN)\n",
    "\n",
    "ε (epsilon), min_samples,  ε-neighborhood, core instance\n",
    "\n",
    "1. For each instance, the algorithm counts how many instances are located within a small distance ε (epsilon) from it. This region is called the instance’s ε-neighborhood.\n",
    "2. If an instance has at least min_samples instances in its ε-neighborhood (including itself), then it is considered a core instance.\n",
    "3. All instances in the neighborhood of a core instance belong to the same cluster.\n",
    "4. Any instance that is not a core instance and does not have one in its neighborhood is considered an anomaly.\n",
    "\n",
    "---\n",
    "\n",
    "### Gaussian Mixtures\n",
    "\n",
    "A probabilistic model that assumes that the instances were generated from a mixture of several Gaussian distributions whose parameters are unknown."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cuda118",
   "language": "python",
   "name": "cuda118"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
