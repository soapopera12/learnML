{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b986c1dc-f074-4590-9876-4ff187e07f4b",
   "metadata": {},
   "source": [
    "There are many API’s in pytorch\n",
    "\n",
    "some important API’s are torch, torch.nn, torch.nn.functional, torch.tensor\n",
    "\n",
    "In python functions can be stored in variables it is called as ***first class functions***\n",
    "\n",
    "```markdown\n",
    "criterion = nn.MSELoss()\n",
    "```\n",
    "\n",
    "`nn.Modules` are defined as Python classes and have attributes, e.g. a `nn.Conv2d` module will have some internal attributes like `self.weight`. `F.conv2d` however just defines the operation and needs all arguments to be passed (including the weights and bias). Internally the modules will usually \n",
    "call their functional counterpart in the `forward` method somewhere.\n",
    "\n",
    "What is torch_geometric.transforms\n",
    "\n",
    "Imagine you have a graph representing a social network, where each node represents a person, and edges represent friendships. Each node has features such as age, number of friends, and number of posts.\n",
    "\n",
    "Before training a GNN, you might want to normalize the node features so that they have a mean of 0 and a standard deviation of 1. This helps the model to train more efficiently and effectively.\n",
    "Here's how you can use `torch_geometric.transforms` to normalize node features:\n",
    "\n",
    "```markdown\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "# Example graph data\n",
    "edge_index = torch.tensor([[0, 1, 1, 2],\n",
    "                           [1, 0, 2, 1]], dtype=torch.long)\n",
    "x = torch.tensor([[10, 1],  # Node 0: age 10, 1 friend\n",
    "                  [20, 2],  # Node 1: age 20, 2 friends\n",
    "                  [30, 3]], # Node 2: age 30, 3 friends\n",
    "                 dtype=torch.float)\n",
    "\n",
    "# Create a graph data object\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# Print original node features\n",
    "print(\"Original node features:\")\n",
    "print(data.x)\n",
    "\n",
    "# Apply the NormalizeFeatures transformation\n",
    "transform = NormalizeFeatures()\n",
    "data = transform(data)\n",
    "\n",
    "# Print normalized node features\n",
    "print(\"\\nNormalized node features:\")\n",
    "print(data.x)\n",
    "```\n",
    "\n",
    "**How is data handled in a graph?**\n",
    "\n",
    "generally graph data is stored as follows\n",
    "data # Data(x=[3700550, 17], edge_attr=[4300999], y=[3700550], train_mask=[857899], valid_mask=[183862], test_mask=[183840], adj_t=[3700550, 3700550, nnz=7994520])\n",
    "data.x.shape  # 3.7M nodes and their 17 features\n",
    "represents the data format typically used for graph data, such as in graph neural networks (GNNs) libraries like PyTorch Geometric (PyG)\n",
    "Node Attributes (x), Edge Attributes (edge_attr), Labels (y), Train/Valid/Test Masks (train_mask, valid_mask, test_mask), Adjacency Matrix (adj_t)\n",
    "\n",
    "**what is adj_t in graph data?**\n",
    "\n",
    "adj_t is a sparse matrix that represents graph structure \n",
    "\n",
    "the row and the column provides the indices where the non-zero(nnz) elements are \n",
    "\n",
    "```\n",
    "SparseTensor(row=tensor([      0,       1,       1,  ..., 3700548, 3700549, 3700549]),\n",
    "             col=tensor([ 146154, 1746169, 1752576,  ..., 2626653, 3249617, 3464387]),\n",
    "             size=(3700550, 3700550), nnz=7994520, density=0.00%)\n",
    "```\n",
    "\n",
    "**What is edge index?**\n",
    "\n",
    "The `edge_index` is typically a 2-dimensional tensor or array that specifies the connections between nodes in a graph. It contains two rows:\n",
    "\n",
    "- The first row lists the source nodes of the edges.\n",
    "- The second row lists the target nodes of the edges.\n",
    "\n",
    "It is the same as adj_t\n",
    "\n",
    "**what is model.eval() ?**\n",
    "\n",
    "In PyTorch, models can behave differently during training (`model.train()`) and evaluation (`model.eval()`). Specifically, some layers like dropout and batch normalization behave differently during training and evaluation phases. Setting the model to evaluation mode ensures these layers behave appropriately for inference, where you don't want random dropout or batch statistics updates.\n",
    "\n",
    "**how to visualize?**\n",
    "\n",
    "We can use TSNE or networkx to visualize as it reduces the dimension from very high dimension to low dimension(like 2d) for visualizing as a plotted points\n",
    "\n",
    "**What about the inverse of the degree matrix in GCN formula?**\n",
    "\n",
    "It is used for normalization purpose so that nodes with higher degrees do not suffer\n",
    "\n",
    "node2vec (shallow node embeddings) or GNN (deep node embeddings) models?\n",
    "\n",
    "node2vec can be used for getting initial embedding however GNN can be used to do a proper task like node classification, graph classification or clustering etc. Also node2vec can be used to get embeddings in a way that neighbor nodes have similar embedding than far away nodes.\n",
    "\n",
    "**what is neighbor explosion?**\n",
    "\n",
    "The embedding of a given node depends recursively on all its neighbor’s embeddings, leading to high inter-dependency between nodes that grows exponentially with respect to the number of layers. This phenomenon is often referred to as *neighbor explosion*.\n",
    "\n",
    "what is the purpose of cross validation?\n",
    "\n",
    "That k-fold cross validation is a procedure used to estimate the skill of the model on new data.\n",
    "\n",
    "The fitting process optimizes the model parameters to make the model fit the training data as well as possible. If we then take an independent sample of validation data from the same population as the training data, it will generally turn out that the model does not fit the validation data as well as it fits the training data. The size of this difference is likely to be large especially when the size of the training data set is small, or when the number of parameters in the model is large. Cross-validation is a way to estimate the size of this effect.\n",
    "\n",
    "\n",
    "Do we pass indices or mask to neighborloader?\n",
    "\n",
    "https://medium.com/stanford-cs224w/a-tour-of-pygs-data-loaders-9f2384e48f8f Here it says we use masks but on official PyG documentation says we use indices!\n",
    "\n",
    "It works for both\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cuda118",
   "language": "python",
   "name": "cuda118"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
