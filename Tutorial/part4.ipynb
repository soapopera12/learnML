{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c18d1db-ea53-4788-b72f-403764d0020e",
   "metadata": {},
   "source": [
    "## Artificial Neural Network\n",
    "\n",
    "---\n",
    "\n",
    "### The perceptron\n",
    "\n",
    "Threshold logic unit (TLU)\n",
    "\n",
    "1. The inputs and output are numbers (instead of binary on/off values), and each input connection is associated with a weight.\n",
    "2. The TLU first computes a linear function of its inputs.\n",
    "3. Then it applies a step function to the result.\n",
    "4. It’s almost like logistic regression, except it uses a step function instead of the logistic (sigmoid) function.\n",
    "\n",
    "A perceptron is composed of one or more TLUs organized in a single layer.\n",
    "\n",
    "Remember each neuron has a bias value\n",
    "\n",
    "---\n",
    "\n",
    "### Backpropagation for perceptron\n",
    "\n",
    "1. The weights of the perceptron are initialized randomly or with small random values.\n",
    "2. The input data is fed into the network, and calculations are carried out layer by layer from the input layer to the output layer to produce the output.\n",
    "    1. The input values are multiplied by their respective weights.\n",
    "    2. These products are summed, and the sum is passed through an activation function to produce the output.\n",
    "3. The error (or loss) is calculated by comparing the network’s output with  the actual target value using a loss function. A common loss function for a single output is the mean squared error (MSE)\n",
    "4. Backpropagation involves three main steps\n",
    "    1. Calculate the gradient: The gradient of the error with respect to each weight is calculated using the chain rule of calculus. This involves determining how changes in weights affect the error.\n",
    "    2. Update the Weights:  The weights are updated in the direction that reduces the error, which is opposite to the gradient. \n",
    "    3. Iterate.\n",
    "\n",
    "---\n",
    "\n",
    "### Backpropagation for multi-layer perceptron\n",
    "\n",
    "**Solved Example Back Propagation Algorithm Multi-Layer Perceptron Network by Dr. Mahesh Huddar**\n",
    "\n",
    "https://www.youtube.com/watch?v=tUoUdOdTkRw\n",
    "\n",
    "Forward pass\n",
    "\n",
    "Backward pass\n",
    "\n",
    "Chain rule\n",
    "\n",
    "---\n",
    "\n",
    "### Activation functions\n",
    "\n",
    "1. **Heaviside**: step function → 0 or 1\n",
    "2. **Tanh**: S shaped → -1 to 1 → \n",
    "3. **Sigmoid**: S shaped → 0 to 1 → logistic function → if you need gradient (because step function has no gradient so no progress in gradient descent)\n",
    "    \n",
    "    If you want to guarantee that the output will always fall within a given range of values, then use the Sigmoid function.\n",
    "    \n",
    "4. **Rectified Linear Unit (ReLU)** : to  get positive output only  →  _/ shaped function\n",
    "    \n",
    "    If you want to guarantee that the output will always be positive, then use the ReLU activation function.\n",
    "    \n",
    "5. **Leaky ReLU:**  Slope z < 0 to ensure that leaky ReLU never dies.\n",
    "6. **Randomized Leaky ReLU (RReLU):** The slope towards the negative side can be either 0, +ve or -ve depending on a hyperparameter.\n",
    "7. **Parametric Leaky ReLU (PReLU):** The slope towards the -ve side is decided by a hyperparameter.\n",
    "8.  **Exponential Linear unit (ELU):** performed better than all ReLU’s with lesser training time and better results only disadvantage being it is slower to compute than ReLU.\n",
    "9. **Scaled Exponential Linear Unit (SELU):** about 1.05 times ELU\n",
    "10. **Gaussian error linear unit** **(GELU):** other activation function discussed above but is computationally intensive.\n",
    "11. **Swish and Mish: other** variants of ReLU. \n",
    "\n",
    "---\n",
    "\n",
    "TensorFlow playground → understanding MLPs → effect of hyperparameters (number of layers, neurons, activation function and more)\n",
    "\n",
    "---\n",
    "\n",
    "**How to decide the number of neurons for each layer in a neural network?**\n",
    "\n",
    "https://medium.com/geekculture/introduction-to-neural-network-2f8b8221fbd3\n",
    "\n",
    "- The number of hidden neurons should be between the size of the input layer and the size of the output layer.\n",
    "- The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.\n",
    "- The number of hidden neurons should be less than twice the size of the input layer.\n",
    "\n",
    "GridSearchCV\n",
    "\n",
    "RandomizedSearchCV\n",
    "\n",
    "---\n",
    "\n",
    "### The Vanishing/Exploding Gradients Problems\n",
    "\n",
    "If weights are initialized with high variance even if i/p has low variance the o/p of a layer can have greater variance. The variance of the outputs of each layer is much greater than the variance of its inputs.\n",
    "\n",
    "Therefore the gradient can be close to 0 leading to vanishing gradients.\n",
    "\n",
    "So how to solve this problem?\n",
    "\n",
    "So we have to solve 2 problems \n",
    "\n",
    "1. How to initialize the weights?\n",
    "2. Which activation function to use?\n",
    "\n",
    "Solving problem 1\n",
    "\n",
    "**Glorot and He Initialization** \n",
    "\n",
    "For weight initialization :- \n",
    "\n",
    "Main idea \n",
    "\n",
    "1. The variance of the outputs of each layer to be equal to the variance of its inputs\n",
    "2. The gradients should have equal variance before and after flowing through a layer in the reverse direction\n",
    "- Fan-in: The number of input units to a layer.\n",
    "- Fan-out: The number of output units from a layer.\n",
    "\n",
    "Therefore connection weights should be initialized randomly → Glorot initialization\n",
    "\n",
    "- **Mean**: The mean of the weights is typically 0.\n",
    "- **Variance**: The variance of the weights is set to:\n",
    "    \n",
    "    Var(W) = $\\sigma^{2} = \\frac{2}{ fan_{in} + fan_{out}}$\n",
    "    \n",
    "\n",
    "This will balance the the variance between the i/p and o/p layer  uniform.\n",
    "\n",
    "solving problem 2\n",
    "\n",
    "Use Relu or other variants of Relu\n",
    "\n",
    "**Batch Normalization**\n",
    "\n",
    "However solving these two problems still does not ensure that the vanishing/exploding gradients problem does not reoccur during training. \n",
    "\n",
    "To address this we have batch normalization.\n",
    "\n",
    "This is done just before or after the activation function in each layer\n",
    "\n",
    "Steps in BN:\n",
    "\n",
    "1. Compute the Mean and Variance: For a given mini-batch, compute the mean and variance of the activations.\n",
    "    \n",
    "    $\\mu_{batch} = \\frac{1}{m} \\sum_{i=1}^{m}x_i$\n",
    "    \n",
    "    $\\sigma_{batch}^2 = \\frac{1}{m}\\sum_{i=1}^{m}(x_i - \\mu_{batch})^2$\n",
    "    \n",
    "2. Normalize the Activations: Subtract the mean and divide by the standard deviation to normalize the activations.\n",
    "    \n",
    "    $\\hat{x_i} = \\frac{x_i - \\mu_{batch}}{\\sqrt{\\sigma_{batch}^2 + \\epsilon}}$\n",
    "    \n",
    "3. **Scale and Shift**: Introduce two trainable parameters, γ (scale) and β (shift), to allow the model to learn the optimal scaling and shifting of the normalized activations.\n",
    "    \n",
    "    $y_i = \\gamma \\times \\hat{x_i} + \\beta$     → for each training instances and for each feature\n",
    "    \n",
    "\n",
    "BN performs scaling. \n",
    "\n",
    "BN also acts like a regularization thus eliminating the need of regularization techniques.\n",
    "\n",
    "**Gradient Clipping**\n",
    "\n",
    "used in RNN for exploding gradients mostly\n",
    "\n",
    "clip the gradients during backpropagation so that they never exceed some threshold.\n",
    "\n",
    "---\n",
    "\n",
    "**Reusing  pretrained models**\n",
    "\n",
    "1. Transfer learning\n",
    "    \n",
    "    If the task at hand is similar to a task that is already solve by a deep neural network (DNN) we can use some or many layer of the existing DNN to help increase the accuracy of the model. We can freeze the reused layers in the first few epochs so that out model are also able to learn and adjust. However it is usally very difficult to find good good configurations and is generally used only in CNNs.\n",
    "    \n",
    "2. Unsupervised Pretraining\n",
    "    \n",
    "    If you did not find any model trained on a similar task use GNN or autoencoders (RBMs)\n",
    "    \n",
    "3. Pretraining on an Auxiliary Task\n",
    "    \n",
    "    If not much labelled training data is present first train a neural network on an auxiliary task for which you can easily obtain or generate labeled training data, then reuse the lower layers of that network for your actual task.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "### Faster Optimizers\n",
    "\n",
    "Till now we have only used SGD where we simply udate the parameters based on the derivation values.\n",
    "But we can speed up this process by using different optimizers\n",
    "\n",
    "1. Momemtum\n",
    "    \n",
    "    A bowling ball rolling down a gentle slope on a smooth surface: it will start out slowly, but it will quickly pick up momentum until it eventually\n",
    "    reaches terminal velocity.\n",
    "    \n",
    "    $\\beta$ is a hyperparameter, $\\alpha$ is the learning rate both are set during training\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9) ← like this\n",
    "    \n",
    "    1. Initialize Velocity (accumulated gradient)\n",
    "    $v_t = 0$\n",
    "    2. Compute gradient\n",
    "    $g_t = \\nabla_\\theta J(\\theta_t)$\n",
    "    3. Update the velocity\n",
    "    $v_t = \\beta v_{t-1} + (1 - \\beta) g_t$\n",
    "    4. Update the parameters\n",
    "    $\\theta_{t+1} = \\theta_t - \\alpha v_t$\n",
    "\n",
    "1. Nesterov Accelerated Gradient\n",
    "    \n",
    "    It measures the gradient of the cost function slightly ahead in the direction of the momentum\n",
    "    \n",
    "    1. Initialize Velocity (accumulated gradient)\n",
    "    $v_t = 0$\n",
    "    2. Lookahead position\n",
    "    $\\theta_{lookahead} = \\theta_t - \\beta v_{t-1}$\n",
    "    3. Compute gradient\n",
    "    $g_t = \\nabla_\\theta J(\\theta_{lookahead})$\n",
    "    4. Update the velocity\n",
    "    $v_t = \\beta v_{t-1} + \\alpha g_t$\n",
    "    5. Update the parameters\n",
    "    $\\theta_{t+1} = \\theta_t - v_t$\n",
    "\n",
    "1. Adagrad\n",
    "    \n",
    "    It maintains a running sum of the squares of the gradients for each parameter. \n",
    "    \n",
    "    1. Initialize Accumulated Squared Gradients\n",
    "    $G_t = 0$\n",
    "    2. Compute gradient\n",
    "    $g_t = \\nabla_\\theta J(\\theta_t)$\n",
    "    3. Update Accumulated Squared Gradients\n",
    "    $G_t = G_{t-1} + g_t^2$\n",
    "    4. Update parameters\n",
    "    $\\theta_{t-1} = \\theta_t - \\frac{\\alpha}{\\sqrt{G_t + \\epsilon}\n",
    "    } \\bigodot  g_t$\n",
    "    \n",
    "    Here, α is the initial learning rate, ϵ is a small constant to prevent division by zero, and ⊙ denotes element-wise multiplication.\n",
    "    \n",
    "\n",
    "1. RMSProp\n",
    "    \n",
    "    AdaGrad runs the risk of slowing down a bit too fast and never converging to the global optimum\n",
    "    \n",
    "    1. Initialize Accumulated Squared Gradients\n",
    "    $E[g^2]_t = 0$\n",
    "    $E[g^2]_t$   is the exponentially decaying average of past squared gradients at time step t.\n",
    "    2. Compute gradient\n",
    "    $g_t = \\nabla_\\theta J(\\theta_t)$\n",
    "    3. Update accumulated squared gradient\n",
    "    $E[g^2]_t = \\beta E[g^2]_{t-1} + (1 - \\beta) g_t^2$\n",
    "    4. Update parameters\n",
    "    $\\theta_{t-1} = \\theta_t - \\frac{\\alpha}{\\sqrt{E[g^2]_t  + \\epsilon}\n",
    "    } \\bigodot  g_t$\n",
    "    α is the learning rate, ϵ is a small constant to prevent division by zero, and ⊙ denotes element-wise multiplication.\n",
    "\n",
    "1. Adam\n",
    "    \n",
    "    a. Initialize Moment Estimates and Time Step\n",
    "    \n",
    "    $m_t = 0$ ( First moment estimate )\n",
    "    \n",
    "    $v_t = 0$  (Second moment estimate)\n",
    "    \n",
    "    t = 0  (Time step)\n",
    "    \n",
    "    b. Compute Gradient\n",
    "    $g_t = \\nabla_\\theta J(\\theta_t)$\n",
    "    \n",
    "    c. Update Time Step \n",
    "    \n",
    "    t = t + 1\n",
    "    \n",
    "    d. Update Biased First Moment Estimate\n",
    "    \n",
    "    $m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)g_t^2$\n",
    "    \n",
    "    $\\beta_1$ is the decay rate for the first moment estimate (typically around 0.9).\n",
    "    \n",
    "    e. Update Biased Second Moment Estimate\n",
    "    \n",
    "    $v_t = \\beta_2 c_{t-1} + (1 - \\beta_2)g_t^2$\n",
    "    \n",
    "    $\\beta_2$ is the decay rate for the second moment estimate (typically around 0.999).\n",
    "    \n",
    "    f. Compute Bias-Corrected First Moment Estimate\n",
    "    \n",
    "    $\\hat{m_t} = \\frac{m_t}{1 - \\beta_1^t}$\n",
    "    \n",
    "    g. Compute Bias-Corrected Second Moment Estimate\n",
    "    \n",
    "    $\\hat{v_t} = \\frac{v_t}{1 - \\beta_2^t}$\n",
    "    \n",
    "    h. Update Parameters\n",
    "    \n",
    "    $\\theta_{t-1} = \\theta_t - \\frac{\\alpha \\hat{m_t}}{\\sqrt{\\hat{v_t}} + \\epsilon}$\n",
    "    \n",
    "    Here, α is the learning rate, and ϵ is a small constant to prevent division by zero.\n",
    "    The below are variations of adam only…\n",
    "    \n",
    "2. AdaMax\n",
    "3. Nadam\n",
    "4. AdamW\n",
    "\n",
    "---\n",
    "\n",
    "**Learning Rate Scheduling**\n",
    "\n",
    "starting with a large learning rate and then reducing it once training stops making fast progress is better than a constant learning rate\n",
    "\n",
    "or start with a low learning rate, increase it, then drop it again. These strategies are called learning schedules.\n",
    "\n",
    "1. Power scheduling\n",
    "    \n",
    "    $\\alpha_t = \\frac{\\alpha_0}{(1 + kt)^\\gamma}$\n",
    "    \n",
    "    - $\\alpha_t$ is the learning rate at time step t.\n",
    "    - $\\alpha_0$ is the initial learning rate.\n",
    "    - k is a hyperparameter that controls how quickly the learning rate decays.\n",
    "    - γ is the power factor that determines the rate of decay (usually between 0 and 1).\n",
    "    - t is the current time step (or epoch).\n",
    "\n",
    "1. Piecewise constant scheduling\n",
    "    - $\\eta_0$ as the initial learning rate\n",
    "    - $\\eta_i$ as the learning rate at the i-th interval\n",
    "    - $T_i$ as the epoch or iteration at which the learning rate changes\n",
    "    \n",
    "    $\\eta(t) = \\eta_i$ for $T_{i-1} < t < T_i$\n",
    "    \n",
    "2. Performance scheduling\n",
    "    \n",
    "    Define\n",
    "    \n",
    "    - η(t) as the learning rate at time t (epoch or iteration)\n",
    "    - $\\eta_{new}$ as the updated learning rate\n",
    "    - ρ as the reduction factor (a value between 0 and 1).\n",
    "    - metric(t) as the performance metric at time t (e.g., validation loss or accuracy).\n",
    "    - patience as the number of epochs or iterations to wait before reducing the learning rate after the performance metric stops improving.\n",
    "    - min_delta as the minimum change in the monitored metric to qualify as an improvement.\n",
    "    \n",
    "    Learning rate is updated as follows\n",
    "    \n",
    "    1. Initialize the learning rate to $\\eta_0$.\n",
    "    2. For each epoch or iteration t track the best performance metric observed so far\n",
    "    if → metric(t) − best_metric > min_delta:\n",
    "        1. Update best_metric to metric(t) \n",
    "        2. Reset epochs_since_improvement to 0.\n",
    "        \n",
    "        Else:\n",
    "        \n",
    "        1.  Increment epochs_since_improvement\n",
    "        2. If epochs_since_improvement exceeds patience:\n",
    "        3. Update the learning rate $\\eta(t)$ to $\\eta_{new} = \\eta(t) \\times \\rho$\n",
    "        4. Reset epochs_since_improvement to 0.\n",
    "\n",
    "1. Exponential scheduling\n",
    "    - $\\eta_0$ as the initial learning rate.\n",
    "    - γ as the decay rate, a constant between 0 and 1.\n",
    "    - t as the current time step (epoch or iteration).\n",
    "    \n",
    "    Learning rate is given by \n",
    "    \n",
    "    $\\eta(t) = \\eta_0 . \\gamma^t$\n",
    "    \n",
    "2. 1cycle scheduling\n",
    "    \n",
    "    It follows a cyclical pattern with a single cycle, starting from an initial value, increasing to a maximum value, and then decreasing back to a minimum value.\n",
    "    \n",
    "    - $\\eta_{min}$ as the initial minimum learning rate.\n",
    "    - $\\eta_{max}$ as the maximum learning rate.\n",
    "    - T as the total number of iterations or epochs.\n",
    "    - t as the current time step (iteration or epoch).\n",
    "    - phase_1_end as the time step at the end of the first phase (halfway point).\n",
    "    \n",
    "    Learning rate at any time t is given by\n",
    "    \n",
    "    if  $t \\le phase\\_1\\_end$  then $\\eta_{min} + \\frac{t}{phase\\_1\\_end} (\\eta_{max} - \\eta_{min})$\n",
    "    \n",
    "    if  $t \\le phase\\_1\\_end$  then $\\eta_{min} + \\frac{t}{phase\\_1\\_end} (\\eta_{max} - \\eta_{min})$\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "### Regularization for Neural Networks\n",
    "\n",
    "Models are prone to overfitting as there are a lot of parameters, regularization can be used to prevent this\n",
    "\n",
    "Early stopping and batch normalization are already acting as regularizers.\n",
    "\n",
    "1. L1 and L2 regularization\n",
    "2. Dropout\n",
    "    \n",
    "    Working of Dropout\n",
    "    \n",
    "    At every training step, every neuron has a probability p of being temporarily “dropped out”, meaning it will be entirely ignored during this training step, but it may be active during the next step. The hyper parameter p is called the dropout rate, and it is typically set between 10% and 50%: closer to 20%–30% in recurrent neural nets, and closer to 40%–50% in convolutional neural networks.\n",
    "    \n",
    "    Neurons cannot co-adapt with neighboring neurons and they have to be as useful as possible on their own. Typically, dropout is turned off during inference, where all neurons are used to make predictions.\n",
    "    \n",
    "3. Monte Carlo (MC) Dropout\n",
    "    \n",
    "    **Training Phase**\n",
    "    \n",
    "    1. Apply dropout to the neural network as usual with a dropout rate p.\n",
    "    2. Train the model on the training data with the standard optimization process.\n",
    "    \n",
    "    **Inference Phase with MC Dropout**\n",
    "    \n",
    "    1. Enable dropout during the inference phase (which is normally turned off).\n",
    "    2. Perform multiple stochastic forward passes through the network for each input sample. Let’s say we perform N forward passes.\n",
    "    3. Each forward pass results in a different set of neurons being dropped out, creating an ensemble of N different predictions for each input. After which the mean of the different results and the variance (uncertainty) can we checked which can be used for confidence assessment.\n",
    "4. Max-Norm Regularization\n",
    "    - W be the weight matrix of a particular layer.\n",
    "    - ∥W∥ be the norm of the weight matrix.\n",
    "    - c be the maximum allowed norm.\n",
    "    \n",
    "    Procedure\n",
    "    \n",
    "    1. Initialize the weights of the neural network.\n",
    "    2. Define the maximum norm threshold c.\n",
    "    3. During training, after each weight update, check the norm of the weight matrix\n",
    "        \n",
    "        If ∥W∥>c, rescale W to have a norm of c\n",
    "        \n",
    "        If the norm of the weight matrix exceeds the threshold cc, the weights are rescaled as follows:\n",
    "        \n",
    "        $W \\leftarrow W . \\frac{c}{|W|}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EnvCuda118",
   "language": "python",
   "name": "envcuda118"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
