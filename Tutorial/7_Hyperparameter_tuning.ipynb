{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ec0e397-db2c-4936-8243-26878dd7b7d9",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "Parameters are like the weights of the model like W and b they are learned during training.\n",
    "\n",
    "However hyperparameters are set prior to training process and govern the training process itself eg., learning rate, number of trees in a random forest.\n",
    "\n",
    "Common hyperparameters\n",
    "\n",
    "1. learning rate\n",
    "2. Batch size\n",
    "3. Number of epochs\n",
    "4. Number of layers and neurons in neural netowork\n",
    "5. Regularization parameters(L1/L2).\n",
    "6. Number of trees in random forests.\n",
    "\n",
    "### Grid Search\n",
    "\n",
    "Grid search involves specifying a set of values for each hyperparameter, and then exhaustively trying all possible combinations of these values to find the best-performing set.\n",
    "\n",
    "Example:\n",
    "For a Support Vector Machine (SVM) with hyperparameters `C` (regularization parameter) and `gamma` (kernel coefficient), a grid search might look like this:\n",
    "\n",
    "```markdown\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=2)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "```\n",
    "\n",
    "### Random Search\n",
    "\n",
    "Random search involves sampling a fixed number of hyperparameter combinations from a specified distribution. This can be more efficient than grid search, especially when dealing with a large number of hyperparameters or a large range of possible values.\n",
    "\n",
    "```markdown\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=RandomForestClassifier(), param_distributions=param_dist, n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(random_search.best_params_)\n",
    "print(random_search.best_estimator_)\n",
    "\n",
    "```\n",
    "\n",
    "### Bayesian Optimization\n",
    "\n",
    "Bayesian optimization builds a probabilistic model of the objective function and uses it to select the most promising hyperparameters to evaluate in the true objective function. It balances exploration and exploitation to efficiently find the optimum hyperparameters.\n",
    "\n",
    "**Example**:\n",
    "Using the `bayes_opt` library for Bayesian Optimization:\n",
    "\n",
    "```markdown\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def rf_cv(n_estimators, min_samples_split, max_features):\n",
    "    val = cross_val_score(\n",
    "        RandomForestClassifier(n_estimators=int(n_estimators), min_samples_split=int(min_samples_split), max_features=min(max_features, 0.999)),\n",
    "        X_train, y_train, scoring='accuracy'\n",
    "    ).mean()\n",
    "    return val\n",
    "\n",
    "pbounds = {\n",
    "    'n_estimators': (10, 200),\n",
    "    'min_samples_split': (2, 10),\n",
    "    'max_features': (0.1, 0.999)\n",
    "}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=rf_cv,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "optimizer.maximize(init_points=10, n_iter=30)\n",
    "\n",
    "print(optimizer.max)\n",
    "\n",
    "```\n",
    "\n",
    "### Gradient-Based Optimization\n",
    "\n",
    "Some advanced techniques use gradient information to adjust hyperparameters. This is common in deep learning frameworks where techniques like learning rate schedules, adaptive learning rates (Adam, RMSprop), and learning rate warm-ups are applied.\n",
    "\n",
    "Best practice\n",
    "\n",
    "1. Start Simple: Begin with a simple model and a limited hyperparameter space. Gradually expand as needed.\n",
    "2. Cross-Validation: Use cross-validation to ensure that the hyperparameter tuning process is robust and the results are not due to overfitting.\n",
    "3. Early Stopping: Implement early stopping to avoid overfitting in deep learning models.\n",
    "4. Learning Curves: Plot learning curves to diagnose the learning process and adjust hyperparameters accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8dd332-1c6e-4a88-b33a-87bb1ea777e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cuda118",
   "language": "python",
   "name": "cuda118"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
