## Natural Language Processing with RNNs and Attention

### word2vec

CBOW  → given neighbor nodes predict the current word → like fill in the blanks

Skip-gram → given a word predict surrounding words or next words

Training word2vec

1. Input layer: Takes in the one-hot encoded vector of a word [ from the whole vocabulary] - length is same as the number of words in vocab
2. Hidden layer: The weights of this layer become the word embeddings → 300 neurons 
3. Output layer: Used to predict the context words or target word depending on the training approach (CBOW or skip-gram)

After training the word2vec model the hidden layer weights become the word embedding 

The rest of the model can be discarded only the middle layer is the most important and a pretrained model can used for getting node embeddings


<div>
  <img src="Images/LSTM.png" alt="LSTM" style="width: 500px; height: 300px;">
</div>



## GloVe

## FastText
