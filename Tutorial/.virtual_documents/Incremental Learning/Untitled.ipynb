# Incremental Learning

# Elastic Weight Consolidation

Denote parameters of layers of a deep neural network (DNN) with Î¸. Training DNNs generates a mapping between the input distribution space and target distribution space.

This is done by finding out an optimum $\theta$ = $\theta^*$ which results in the least error in the training objective. It has been shown in earlier works that such a mapping can be obtained 

with many configurations of $\theta^*$ like this image.


