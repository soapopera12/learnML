{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d040e469-a384-49f0-9c46-d9a5a74524b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8be90e12-4479-4948-a908-a169972e6a24",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef1c1ad-c59e-41dc-aa63-4de14d76566e",
   "metadata": {},
   "source": [
    "<div>\n",
    "  <img src=\"Images/transformer.png\" alt=\"transformers\" style=\"width: 500px; height: 300px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4c29b0-0d9d-4c0d-8996-3b11f4202aeb",
   "metadata": {},
   "source": [
    "1. Left part\n",
    "    1. Input embedding\n",
    "        \n",
    "        First the input is being fed into the input embedding layer (more of a lookup table) where a word is converted into a vector representation.\n",
    "        \n",
    "    2. Positional Encoding\n",
    "        \n",
    "        As the transformer as no recurrence like a RNN, we must add some information about the position into the input\n",
    "        \n",
    "        For every odd index :    $PE_{(pos,2i+1)} = cos(pos/10000^{(2i/d_{model})})$\n",
    "        \n",
    "        For every even index:  $PE_{(pos,2i)} = sin(pos/10000^{(2i/d_{model})})$\n",
    "        \n",
    "    3. Encoder layer\n",
    "        \n",
    "        It consists of a simple multi-headed attention network followed by a feed-forward network\n",
    "        \n",
    "        The multi-header attention network creates query, key and value vectors from the combination of postional encoding and input embedding.\n",
    "        \n",
    "        The dot product of query and key gives the scores matrix which determines how much should a word focus on every other word that is fed to it.\n",
    "        \n",
    "        higher the score more will be the focus.\n",
    "        \n",
    "        The scores are scaled down by dividing it by square root of the dimension of query and key allowing for stable gradients.\n",
    "        \n",
    "        Next we softmax the scores to get the resulting attention values.\n",
    "        \n",
    "        Finally the scores and the value matrix is multiplied to get the resulting output vector.\n",
    "        \n",
    "        For computing multi-header attention all you need to do is to split the query, key and value vectors into n vectors before applying self attention.\n",
    "        \n",
    "2. Right part\n",
    "    1. output embedding\n",
    "        \n",
    "        It takes the list of previous outputs as inputs.\n",
    "        \n",
    "    2. Positional encoding\n",
    "        \n",
    "        Same as encoder but for output embeddings\n",
    "        \n",
    "    3. Decoder\n",
    "        \n",
    "        It consists of two multi-header attention network followed by a feed-forward network and a linear layer. The output embedding of encoder are fed after the first multi-headed attention layer.\n",
    "        \n",
    "        The first multi-headed attention network is slightly different as we need to ensure that the future words are not feed to it. Each words scores should be computed only for the words coming before it not afterwards. This is done via masking. Basically putting -inf for all score values.\n",
    "        \n",
    "        The final linear layer acts as a classifier and is as big as the number of classes i.e. number of words(vocab size) you have. It will generate a probability value between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bdd722-25e9-4299-b078-7e2f4ccc28e5",
   "metadata": {},
   "source": [
    "## BERT\n",
    "\n",
    "https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61398ad8-44f3-4107-bb82-6c50693613c1",
   "metadata": {},
   "source": [
    "<div>\n",
    "  <img src=\"Images/BERT.png\" alt=\"BERT\" style=\"width: 500px; height: 300px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24303cb-3d07-4b72-af9a-8e6d39a3ff92",
   "metadata": {},
   "source": [
    "Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "Bidirectional: As opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the Transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional.\n",
    "\n",
    "Since BERT’s goal is to generate a language model, only the encoder mechanism is necessary.\n",
    "\n",
    "Training \n",
    "\n",
    "**Masked LM (MLM)** \n",
    "\n",
    "**Before feeding word sequences into BERT**, 15% of the words in each sequence are replaced with a [MASK] token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence. The BERT loss function takes into consideration only the prediction of the masked values and ignores the prediction of the non-masked words.\n",
    "\n",
    "**Next Sentence Prediction (NSP)**\n",
    "\n",
    "In the BERT training process, the model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence. The assumption is that the random sentence will be disconnected from the first sentence.\n",
    "\n",
    "BERT fine-tuning\n",
    "\n",
    "Using BERT for a specific task is relatively straightforward: → done instead of NSP\n",
    "\n",
    "1. Classification tasks such as sentiment analysis are done similarly to Next Sentence classification, by adding a classification layer on top of the Transformer output for the [CLS] token.\n",
    "2. In  Question Answering tasks (e.g. SQuAD v1.1), the software receives a question regarding a text sequence and is required to mark the answer in  the sequence. Using BERT, a Q&A model can be trained by learning two extra vectors that mark the beginning and the end of the answer.\n",
    "3. In Named Entity Recognition (NER), the software receives a text sequence and is required to mark the various types of entities (Person, Organization, Date, etc) that appear in the text.\n",
    "\n",
    "BERT uses encoder-only architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8863de3-58df-460a-8aea-f6ba7f40ba34",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a251c9-3fe8-4bf9-8fd1-0a21df1fb73e",
   "metadata": {},
   "source": [
    "<div>\n",
    "  <img src=\"Images/GPT.png\" alt=\"GPT\" style=\"width: 500px; height: 300px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fdc73f-01b1-4b72-bbd8-c240ba5700cd",
   "metadata": {},
   "source": [
    "Generative Pretrained Transformers\n",
    "\n",
    "Generative: This feature emphasizes the model's ability to generate text by comprehending and responding to a given text sample. \n",
    "\n",
    "Pretrained: refers to the ML model that has undergone training on a large dataset of examples before being deployed for a specific task.\n",
    "\n",
    "Transformers:  ****A type of neural network architecture that is designed to handle text sequences of varying lengths.\n",
    "\n",
    "GPT uses decoder-only architecture.\n",
    "\n",
    "**So whats the difference between BERT and GPT?**\n",
    "\n",
    "- Architecture:\n",
    "    - GPT: Uses only the decoder part of the transformer\n",
    "    - BERT: Uses only the encoder part of the transformer\n",
    "- Pretraining Task:\n",
    "    - GPT: Next token prediction (language modeling)\n",
    "    - BERT: Masked language modeling and next sentence prediction\n",
    "- Attention Mechanism:\n",
    "    - GPT: Unidirectional (can only attend to previous tokens)\n",
    "    - BERT: Bidirectional (can attend to both previous and future tokens)\n",
    "- Input Processing:\n",
    "    - GPT: Processes text left-to-right\n",
    "    - BERT: Can access the full context in both directions\n",
    "- Primary Use:\n",
    "    - GPT: Excels at text completion and conversation.\n",
    "    - BERT: Better suited for understanding tasks (question answering and named entity recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f16d4a9-0366-4231-b0fb-b388405ef03e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cuda118",
   "language": "python",
   "name": "cuda118"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
