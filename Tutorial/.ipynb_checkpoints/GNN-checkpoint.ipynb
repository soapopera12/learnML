{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83238306-03b2-4a3f-b719-14f24aa218b6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54a96992-a66b-4129-9a53-7bf2d2ce0875",
   "metadata": {},
   "source": [
    "## Graph Neural Networks (GNN)\n",
    "\n",
    "### Application of graph machine learning\n",
    "\n",
    "2 types of prediction task → graph-level, node-level, edge-level and community level\n",
    "\n",
    "**Graph-level tasks**\n",
    "\n",
    "Categorize graphs into types → discover new drugs\n",
    "\n",
    "**Node-level tasks**\n",
    "\n",
    "Classification of nodes → protein folding\n",
    "\n",
    "**Edge-level tasks**\n",
    "\n",
    "Missing link prediction → recommendation system like pinSage\n",
    "\n",
    "**Community-level tasks**\n",
    "\n",
    "Clustering task → traffic prediction\n",
    "\n",
    "---\n",
    "\n",
    "### **Graph Representation**\n",
    "\n",
    "1. Directed vs undirected graph.\n",
    "2. Weighted vs unweighted graph.\n",
    "3. Connected vs unconnected graph.\n",
    "4. Self loops and multi-graph.\n",
    "5. Bipartite graph.\n",
    "6. Adjacency matrix.\n",
    "7. Edge list.\n",
    "8. Adjacency list.\n",
    "\n",
    "---\n",
    "\n",
    "### Traditional methods for Machine learning in Graphs\n",
    "\n",
    "given a graph extract node, link and graph level features and learn a model (SVM, neural network, etc) that maps features to labels\n",
    "\n",
    "Input graph → Structured features → Learning algorithm → Prediction\n",
    "\n",
    "Comes between Input graph and structured features → Feature engineering (node-level features, edge-level features, graph-level features)  \n",
    "\n",
    "**Node-level features**\n",
    "\n",
    "The following are node level features\n",
    "\n",
    "**Node degree** \n",
    "\n",
    "**Node centrality**\n",
    "\n",
    "1. **Eigen vector centrality**\n",
    "    \n",
    "    For a node v, it’s centrality is how many important neighbor node it has u $\\in$  N(v), where N(v) are neighbors of v\n",
    "    \n",
    "    $c_v = \\frac{1}{\\lambda} \\sum_{u \\in N(v)} c_u$ \n",
    "    \n",
    "    where $\\lambda$ is some positive constant. \n",
    "    \n",
    "2. **Betweenness centrality**\n",
    "    \n",
    "    If a node lies in many shortest paths between other nodes\n",
    "    \n",
    "    $c_v = \\sum_{s \\neq v \\neq t} \\frac{Number \\ of \\ shortest \\ paths \\ between \\ s \\ and \\ t \\ that \\ contain \\ v}{Number \\ of \\ paths \\ between \\ s \\ and \\ t}$\n",
    "    \n",
    "3. **Closeness centrality**\n",
    "    \n",
    "    smallest shortest path lengths to all other nodes\n",
    "    \n",
    "    $c_v = \\frac{1}{\\sum_{u \\neq v} shortest \\ path \\ length \\ between \\ u \\ and \\ v}$\n",
    "    \n",
    "\n",
    "**Cluster coefficient**\n",
    "\n",
    "measure how connected v’s neighboring nodes are\n",
    "\n",
    "$e_v = \\frac{Number \\ of \\ edges \\ among \\ neighboring\\ nodes}{{k_v\\choose 2}} \\in [0,1]$\n",
    "\n",
    "$k_v \\choose 2$ is the number of node pairs among $k_v$ neighboring nodes\n",
    "\n",
    "Measures how connected one’s neighboring nodes are → clustering coefficient\n",
    "\n",
    "**Graphlets**\n",
    "\n",
    "clustering coefficients counts the number of triangles in the ego network\n",
    "\n",
    "Graphlets are rooted connected non-isomorphic subgraph\n",
    "\n",
    "1. Graphlet degree vector\n",
    "    \n",
    "    Count vector  of graphlets rooted at a given node.\n",
    "    \n",
    "    where degree is the number of edges that a node touches and clustering coefficient counts the number of triangles that a node touches.\n",
    "    \n",
    "    GDV provides a measure of a node’s local network topology.\n",
    "    \n",
    "\n",
    "**Edge-level features**\n",
    "\n",
    "For each pair (x,y) compute score c(x,y), sort these pairs in decreasing order and predict top n pairs as new links and see which of these links actually exist in the graph\n",
    "\n",
    "Types of link level fetures\n",
    "\n",
    "1. Distance based feature.\n",
    "    \n",
    "    shortest path distance between two nodes.\n",
    "    \n",
    "2. Local neighborhood overlap.\n",
    "    \n",
    "    Number of common neighbors between two nodes\n",
    "    \n",
    "    1. Common neighbors \n",
    "        \n",
    "        $N(v_1) \\cap N(v_2)$\n",
    "        \n",
    "    2. Jaccard’s coefficient\n",
    "        \n",
    "        $\\frac{| N(v_1) \\cap N(v_2)|\n",
    "        }{|N(v_1) \\cup N(v_2)|\n",
    "        }$\n",
    "        \n",
    "    3. Adamic-Adar index\n",
    "        \n",
    "        $\\sum_{v \\in N(v_1) \\cap N(v_2)} \\frac{1}{log(k_u)}$ \n",
    "        \n",
    "        here k is the degree\n",
    "        \n",
    "3. Global neighborhood overlap.\n",
    "    \n",
    "    Katz index.\n",
    "    \n",
    "    Count the number of paths of all lengths between a given pair of nodes.\n",
    "    \n",
    "    Computing the number of paths between two nodes = computing the powers of graph’s adjacency matrix\n",
    "    \n",
    "    $P_{uv}^{(k)} =$ Number of paths of length k between u and v \n",
    "    \n",
    "    $P^{(k)} = A^k$\n",
    "    \n",
    "    here $P^{(1)}_{uv}$ is the number of paths of length 1 between u and v in Adjacency matrix\n",
    "    \n",
    "    so katz index between $v_1$ and $v_2$ is calculated as sum over all path lengths\n",
    "    \n",
    "    $S_{v_1v_2} = \\sum_{l=1}^{\\infty} \\beta^l A_{v_1v_2}^l$\n",
    "    \n",
    "    where \n",
    "    \n",
    "    0 < $\\beta$ < 1 is the discount factor\n",
    "    \n",
    "    $A_{v_1v_2}^l$ is the number of paths of length l between $v_1$and $v_2$.\n",
    "    \n",
    "\n",
    "**Graph-level features**\n",
    "\n",
    "Graph kernel and types\n",
    "\n",
    "kernel K($G, G') \\in \\mathbb{R}$  measure similarity between data points.\n",
    "\n",
    "Bag-of-Words (BoW) → key idea → bag-of-*\n",
    "\n",
    "1. Graphlet kernel\n",
    "    \n",
    "    count the number of different graphlets in the graph\n",
    "    \n",
    "    here graphlets are not rooted and don’t have to be connected → different from node level graphlets (check standford for clarification)\n",
    "    \n",
    "    $K(G, G') = f_G^T f_{G'}$\n",
    "    \n",
    "    However if G and G’ have different sizes we have to normalize the feature vector\n",
    "    \n",
    "    $K(G, G') = h_G^T h_{G'}$ where $h_G = \\frac{f_G}{Sum(f_G)}$\n",
    "    \n",
    "    However this can be expensive and is NP-Hard\n",
    "    \n",
    "2. Weisfeiler-Lehman kernel\n",
    "    \n",
    "    Using neighborhood structure\n",
    "    \n",
    "    Color refinement\n",
    "    \n",
    "    A graph G with node V\n",
    "    \n",
    "    Give a color $c^{(0)}(v)$ to each node v\n",
    "    \n",
    "    Then refine node colors by\n",
    "    \n",
    "    $c^{(k+1)}(v) = HASH(\\{c^{(k)}(v), \\{ c^{(k)}(u)\\}_{u \\in N(v)}\\})$    → The refined node color is a hash value of existing node color and color from it’s neighbors\n",
    "    \n",
    "    k is the number of steps → K-hop neighborhood\n",
    "    \n",
    "    After color refinement WL kernel counts nodes with given color\n",
    "    \n",
    "    $K(G, G') = \\phi(WLK)^T \\phi(WLK')^T$ = 49 → or any other value this is the inner product of the color count vectors\n",
    "    \n",
    "    WLK is computationally efficient\n",
    "    \n",
    "3. Random walk kernel\n",
    "4. shortest path graph kernel\n",
    "\n",
    "---\n",
    "\n",
    "**Node Embeddings** \n",
    "\n",
    "Node → represent in the form of a vector of dimension d also called embedding\n",
    "\n",
    "encode(nodes) → embedding space\n",
    "\n",
    "encode(u) → $Z_u$\n",
    "\n",
    "encode(v) → $Z_v$\n",
    "\n",
    "Similarity(u, v) = $z_v^T z_u$ (similarity of embedding) →similarity of nodes correspond to the similarity of embedding\n",
    "\n",
    "encoding approaches\n",
    "\n",
    "Shallow encoding →  encoding is just an embedding-lookup → each node is assigned a unique embedding vector\n",
    "\n",
    "1. DeepWalk\n",
    "2. node2vec\n",
    "\n",
    "decoder → maps from embedding to similarity score\n",
    "\n",
    "maximize $z_v^T z_u$ for node pairs(u,v) that are similar\n",
    "\n",
    "---\n",
    "\n",
    "**Random walk approaches for Node embedding**\n",
    "\n",
    "random walks are expressive (incorporates lower and higher order neighborhood info) and efficient (as only a pair of nodes is considered not whole graph)\n",
    "\n",
    "what to do? → find embedding of a node in d-dimensional space\n",
    "\n",
    "Idea → learn node embedding in a way such that nearby nodes are closer in the network\n",
    "\n",
    "How to define nearby nodes? → $N_R(u)$ neighborhood of u is defined by some random walk strategy R\n",
    "\n",
    "Find vector $z_u$ (embedding) given node u.\n",
    "\n",
    "Probability P(v | $z_u$) → predicted probability of visiting node v on random walks starting from node u.\n",
    "\n",
    "~\n",
    "\n",
    "To find the prediction probability we use non-linear function\n",
    "\n",
    "1. Softmax function\n",
    "    \n",
    "    turns vector of K real values (model prediction) into K probabilities that sum to 1\n",
    "    \n",
    "    $\\sigma(z) = \\frac{e^{z_i}}{\\sum_{j=1}^Ke^{z_j}}$\n",
    "    \n",
    "2. Sigmoid function\n",
    "    \n",
    "    S-shaped function that turns real values between 0 and 1\n",
    "    \n",
    "    $S(x) = \\frac{1}{1+e^{-x}}$\n",
    "    \n",
    "\n",
    "Random walk\n",
    "\n",
    "Sequence of points visited in a random way is called random walk.\n",
    "\n",
    "Random walk embedding\n",
    "\n",
    "$z_v^T z_u$ ⇒ probability that u and v co-occur on a random walk over the graph\n",
    "\n",
    "1. Estimate probability of visiting node v on a random walk starting from node u using some random walk strategy R.\n",
    "2. Optimize embedding to encode these random walk statistics.\n",
    "\n",
    "**Feature learning optimization**\n",
    "\n",
    "G = (V, E)\n",
    "\n",
    "our goal is to learn a mapping f: u → $\\mathbb{R}^d:$\n",
    "\n",
    "f(u) = $z_u$\n",
    "\n",
    "log likelihood objective:\n",
    "\n",
    "$\\max_{f} \\sum_{u \\in V} logP(N_R(u) | z_u)$ → find a function such that for all the nodes in the graph it maximizes the log probability of the nodes in the neighborhood.\n",
    "\n",
    "where $N_R(u)$ is the neighborhood of node u by strategy R\n",
    "\n",
    "1. Run short fixed-length random walks starting from each node u in the graph using some random walk strategy R.\n",
    "2. For each node u collect $N_R(u)$, the multiset* of nodes visited on random walks starting from u.\n",
    "3. Optimize embedding according to: Given node u, predict its neighbors $N_R(u)$ → using SGD\n",
    "    \n",
    "    $\\max_{f} \\sum_{u \\in V} logP(N_R(u) | z_u)$   → maximum likelihood objective\n",
    "    \n",
    "    given the embedding of node u the probability of multi-set $N_R(u)$ is maximized.\n",
    "    \n",
    "    This can also be written as\n",
    "    \n",
    "    $L = \\sum_{u \\in V} \\sum_{v \\in N_R(u)} -log(P(v|z_u))$\n",
    "    \n",
    "    $P(v|z_u) = \\frac{exp(z_u^T.z_v)}{\\sum_{n \\in V}exp(z_u^T.z_{v'})}$ → Why softmax? → node v should be similar to node u out of all nodes n\n",
    "    \n",
    "    This requires each nodes dot product with every other node → which is computationally expensive\n",
    "    \n",
    "    Find embeddings $z_u$ that minimize L\n",
    "    \n",
    "    $\\sum_{n \\in V}exp(z_u^T.z_{v'})$ → this normalization is very expensive\n",
    "    \n",
    "    to tackle this we use negative sampling\n",
    "    \n",
    "    Rather than summing over all the nodes we sum over only a few node that are chosen using negative sampling\n",
    "    \n",
    "    $\\frac{exp(z_u^T.z_v)}{\\sum_{n \\in V}exp(z_u^T.z_{v'})} \\approx log(\\sigma(z_u^T.z_v)) - \\sum_{i=1}^{k}log(\\sigma(z_u^T.z_{n_{i}})), n_i \\sim P_V$\n",
    "    \n",
    "    here k is the number of negative samples\n",
    "    \n",
    "    How to select negative samples?\n",
    "    \n",
    "    Probability of choosing a node is proportional to its degree\n",
    "    \n",
    "    1. higher k gives more robust estimates\n",
    "    2. higher k values correspond to higher bias\n",
    "    \n",
    "    in practice k = 5 to 20\n",
    "    \n",
    "\n",
    "We use SGD to optimize the objective function L\n",
    "\n",
    "$L = \\sum_{u \\in V} \\sum_{v \\in N_R(u)} -log(P(v|z_u))$\n",
    "\n",
    "1. Initialize $z_i$ at some randomized value for all i.\n",
    "2. Iterate until convergence:   $L^{(u)} = \\sum_{v \\in N_R(u)} -log(P(v|z_u))$\n",
    "    1. For all i, compute the derivative $\\frac{\\delta L\n",
    "    }{\\delta z_i}$\n",
    "    2. for all l, make a step towards the direction of derivative: $z_i$ ← $z_i - \\eta \\frac{\\delta L}{\\delta z_i}$\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "**node2vec**\n",
    "\n",
    "like skip-gram model → predict context words → in graph → predict context nodes\n",
    "\n",
    "skip-gram with negative sampling\n",
    "\n",
    "random walks are used to generate context nodes\n",
    "\n",
    "second-order graph traversal\n",
    "\n",
    "Random Walk Generation\n",
    "\n",
    "1. Graph Representation\n",
    "    \n",
    "    Consider a graph G=(V,E) with V as the set of nodes and E as the set of edges.\n",
    "    \n",
    "2. Random Walk Strategy\n",
    "    \n",
    "    Node2Vec introduces two parameters p and q, to control the walk behavior:\n",
    "    \n",
    "    p: Return parameter, controlling the likelihood of revisiting the previous node.\n",
    "    \n",
    "    q: In-out parameter (inout parameter), controlling the likelihood of visiting nodes further away.\n",
    "    \n",
    "3. Transition Probabilities\n",
    "For a walk starting at node t and currently at node v, the next step to node x is determined by the transition probability:\n",
    "    \n",
    "    $\\pi_{vx} = \\alpha_{pq}(t,x).w_{vx}$\n",
    "    \n",
    "    where $w_{vx}$ is the weight of the edge between v and x, and $\\alpha_{pq}(t, x)$ is a bias factor based on p and q.\n",
    "    \n",
    "4. Bias Factor Calculation\n",
    "    \n",
    "    $\\alpha_{pq}(t, x)$ is defined as:\n",
    "    \n",
    "    $\\alpha_{pq}(t, x) = \n",
    "    \\begin{cases}\n",
    "          \\frac{1}{p} \\ if \\ d_{tx} = 0\\\\\n",
    "          1 \\ if \\ d_{tx} = 1 \\\\ \n",
    "          \\frac{1}{q} \\ if \\ d_{tx} = 2 \\\\\n",
    "        \\end{cases}$  \n",
    "    \n",
    "    where $d_{tx}$ is the shortest path distance between t and x\n",
    "    \n",
    "5. Generating Walks\n",
    "For each node u in the graph, generate multiple random walks of fixed length. The walks are influenced by p and q, allowing for a balance between depth-first and breadth-first search strategies.\n",
    "\n",
    "---\n",
    "\n",
    "**Embedding entire graph**\n",
    "\n",
    "Approach 1\n",
    "\n",
    "embedding of graph = sum of embedding of all nodes in the graph\n",
    "\n",
    "Approach 2\n",
    "\n",
    "introduce a subgraph in the graph and find embedding for this sub-graph\n",
    "\n",
    "Approach 3\n",
    "\n",
    "anonymous walk embedding\n",
    "\n",
    "An anonymous walk of length k in a graph is a sequence of integers ($a_1, a_2, ... , a_{k+1}$) where each integer represents the position or role of a node in the walk, not its specific identity. \n",
    "\n",
    "Example of Anonymous Walk\n",
    "\n",
    "Consider a graph with nodes A,B,C,D and a random walk starting from node A with the sequence A→B→C→A→D .\n",
    "\n",
    "1. Original Random Walk:\n",
    "    \n",
    "    sequence A→B→C→A→D \n",
    "    \n",
    "2. Anonymous Walk:\n",
    "3. Position indices: (1,2,3,1,4)\n",
    "    \n",
    "    Explanation:\n",
    "    \n",
    "    - A is visited first (index 1).\n",
    "    - B is visited second (index 2).\n",
    "    - C is visited third (index 3).\n",
    "    - A is revisited, so it retains index 1.\n",
    "    - D is visited for the first time after the revisits, so it gets index 4.\n",
    "\n",
    "The number of anonymous walks grows exponentially\n",
    "\n",
    "How to use anonymous walks?\n",
    "\n",
    "simulate anonymous walks $w_i$ of $l$ steps and record their counts\n",
    "\n",
    "Represent the graph as a probability distribution over these walks\n",
    "\n",
    "example -\n",
    "\n",
    "set $l = 3$\n",
    "\n",
    "Then we can represent the graph as a 5-dimensional vector\n",
    "\n",
    "- since there are 5 anonymous walks $w_i$ of length 3:111, 112, 121, 122, 123\n",
    "\n",
    "$z_G[i] =$ probability of anonymous walk $w_i$ in G\n",
    "\n",
    "How many random walks do we need?\n",
    "\n",
    "we want the distribution to have error of more than $\\epsilon$ with probability less than $\\delta$:\n",
    "\n",
    "$m = [ \\frac{2}{\\epsilon^2} (log(2^\\eta - 2) - log(\\delta) ) ]$\n",
    "\n",
    "where $\\eta$ is the number of anonymous walks of length $l$\n",
    "\n",
    "For example: \n",
    "\n",
    "There are $\\eta$=877 anonymous walks of length 7. If we set $\\epsilon$=0.1 and $\\delta=0.01$ then we need to generate m=122,500 random walks\n",
    "\n",
    "Approach 4\n",
    "\n",
    "Learn graph embedding $Z_G$ together with all the anonymous walk embedding $z_i$ of anonymous walk $w_i$\n",
    "\n",
    "Z = {$z_i:i=1 ... \\eta$}, where $\\eta$ is the number of sampled walks\n",
    "\n",
    "learn vector $Z_G$ for input graph\n",
    "\n",
    "starting from node 1 sample anonymous random walks\n",
    "\n",
    "learn to predict walks that co-occur in $\\Delta$-size window (eg. predict $w_2$ given $w_1,w_3$ if $\\Delta=1$\n",
    "\n",
    "objective:\n",
    "\n",
    "$max \\sum_{t = \\Delta}^{T-\\Delta}logP(w_t|w_{t-\\Delta},...,w_{t-\\Delta},z_G)$\n",
    "\n",
    "sum the objective over all nodes in the graph\n",
    "\n",
    "Once you learn $z_G$ we can use it for graph classification, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87a9430-9d59-42b4-88de-68a2517eef8e",
   "metadata": {},
   "source": [
    "### Graph as a matrix: Pagerank, Random walks and embeddings\n",
    "\n",
    "**Link Analysis Algorithms**\n",
    "\n",
    "1. **Pagerank**\n",
    "    - Measure the importance of nodes in a graph using link structure of the web.\n",
    "    - Models a random web surfer using the stochastic adjacency matrix M.\n",
    "    - Pagerank solves r = M.r    where r can be viewed as both the principle eigenvector of M and as the stationary distribution of random walk over the graph.\n",
    "    \n",
    "    There are 2 types of links → in-links and out-links \n",
    "    \n",
    "    in-links are more important as they are hard to control\n",
    "    \n",
    "    Pagerank\n",
    "    \n",
    "    The vote from important page is worth more:\n",
    "    \n",
    "    - Each link’s vote is proportional to the importance of its source page\n",
    "    - if page i with importance $r_i$ has $d_i$ out-links, each link gets $r_i/d_i$ votes\n",
    "    - page j’s own importance $r_j$ is the sum of the votes on it’s in-links\n",
    "    \n",
    "    $r_j = \\sum_{i \\rightarrow j} \\frac{r_i}{d_i}$    ——— > equation form\n",
    "    \n",
    "    here $d_i$ is the out-degree of node i\n",
    "    \n",
    "    This will give you “Flow” equations:\n",
    "    \n",
    "    $r_y = r_y/2 + r_a/2$\n",
    "    \n",
    "    $r_a = r_y/2 + r_m$\n",
    "    \n",
    "    $r_m = r_a/2$\n",
    "    \n",
    "    How to solve them → we can use gaussian elimination but that is expensive\n",
    "\n",
    "Problems\n",
    "\n",
    "1. Dead ends\n",
    "    \n",
    "    have no out-links\n",
    "    \n",
    "    solution → teleport out with probability 1.0\n",
    "    \n",
    "2. Spider traps\n",
    "    \n",
    "    All out-links are within the group\n",
    "    \n",
    "    solution → teleport out of the spider trap within a few time steps with probability < 1.0\n",
    "    \n",
    "\n",
    "So the pagerank equation becomes\n",
    "\n",
    "$r_j = \\sum_{i \\rightarrow j} \\beta \\frac{r_i}{d_j} + (1 - \\beta) \\frac{1}{N}$   \n",
    "\n",
    "In matrix form\n",
    "\n",
    "P = $\\beta$ M + ($1 - \\beta)[\\frac{1}{N}]_{N \\times N}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e96155b-1815-46fa-97f3-b901a1dc2a80",
   "metadata": {},
   "source": [
    "Lets represent the graph as a matrix\n",
    "\n",
    "Stochastic adjacency matrix M  → this is just the adjacency matrix\n",
    "\n",
    "let page j have $d_j$ out-links\n",
    "\n",
    "if j → i, then $M_{ij}$ = $\\frac{1}{d_j}$                 —————— > here the out-links are divided over 1 (all columns should sum to 1)\n",
    "\n",
    "M is a column stochastic matrix meaning columns sum to 1\n",
    "\n",
    "Rank vector r: an entry per page\n",
    "\n",
    "$r_i$ is the importance score of page i\n",
    "\n",
    "$\\sum_i r_i = 1$     →   the entries of vector r (all the pages) has to sum to 1  →  probability distribution over the nodes in the network\n",
    "\n",
    "The flow equation can be written as\n",
    "\n",
    "$r = M.r$             —— >  in the matrix forum\n",
    "\n",
    "$r_j = \\sum_{i \\rightarrow j}\\frac{r_i}{d_j}$    ——— > equation form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5462ebb3-d007-4a74-9494-ac56fc57f419",
   "metadata": {},
   "source": [
    "<div>\n",
    "  <img src=\"Images/graph.png\" alt=\"graph\" style=\"width: 200px; height: 200px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071a7710-b5ce-48c9-bff1-d3d3e252050e",
   "metadata": {},
   "source": [
    "Equation form\n",
    "\n",
    "$r_y = r_y/2 + r_a/2$\n",
    "\n",
    "$r_a = r_y/2 + r_m$\n",
    "\n",
    "$r_m = r_a/2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1990856-aa8c-44c2-9702-612916ce74a9",
   "metadata": {},
   "source": [
    "Matrix form\n",
    "\n",
    "|  | $r_y$ | $r_a$ | $r_m$ |\n",
    "| --- | --- | --- | --- |\n",
    "| $r_y$ | 1/2 | 1/2 | 0 |\n",
    "| $r_a$ | 1/2 | 0 | 1 |\n",
    "| $r_m$ | 0 | 1/2 | 0 |\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "r_y \\\\ \n",
    "r_a \\\\\n",
    "r_m\n",
    "\\end{bmatrix}  = \n",
    "\\begin{bmatrix}\n",
    "1/2 & 1/2 & 0 \\\\ \n",
    "1/2 & 0 & 1 \\\\\n",
    "0 & 1/2 & 0\n",
    "\\end{bmatrix}  \n",
    "\\begin{bmatrix}\n",
    "r_y \\\\ \n",
    "r_a \\\\\n",
    "r_m\n",
    "\\end{bmatrix}$   \n",
    "\n",
    "$r$                                  $M$                         $r$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3733dd6f-61c8-4c66-9c0b-f1d37d73350f",
   "metadata": {},
   "source": [
    "p(t) vector whose ith coordinate is the probability that the surfer is at page i  at time t\n",
    "\n",
    "so, p(t) is the probability distibution over pages\n",
    "\n",
    "Where is the surfer at time t+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bd621c-4ed9-4d1c-9ba6-d15347b624dd",
   "metadata": {},
   "source": [
    "!IMage not showing\n",
    "<div>\n",
    "  <img src=\"Images/pageRank.png\" alt=\"pageRank\" style=\"width: 500px; height: 500px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b1b207-f4dd-49c9-a5d2-7def8111475e",
   "metadata": {},
   "source": [
    "pick any outlink at uniform and go to it\n",
    "\n",
    "p(t+1) = M. p(t)\n",
    "\n",
    "after some time a random walk reaches a state \n",
    "\n",
    "p(t+1) = M .p(t) = p(t)      …. no change p(t) is a stationary distribution of a random walk\n",
    "\n",
    "our original rank vector r satisfies r = M.r\n",
    "\n",
    "so the flow equation \n",
    "\n",
    "1.r = M.r\n",
    "\n",
    "so the rank vector r is a eigenvector of stochastic adj matrix M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e484a2d1-7948-447f-b6dd-b714c9049a60",
   "metadata": {},
   "source": [
    "How to solve pagerank?\n",
    "\n",
    "- Assign each node an initial page rank.\n",
    "- Repeat until convergence      $( \\sum_i|r_i^{t+1} - r_i^t| < \\epsilon)$\n",
    "    \n",
    "    calculate the page rank of each node\n",
    "    \n",
    "    $r_j^{(t+1)} = \\sum_{i \\rightarrow j} \\frac{r_i^{(t)}}{d_i}$\n",
    "    \n",
    "\n",
    "Solve using Power iteration method\n",
    "\n",
    "- Initialize $r^0 = [1/N,....,1/N]^T$\n",
    "- Iterate $r^{(t+1)} = M.r^t$\n",
    "- stop when $|r^{t+1} - r^t|_1 < \\epsilon$\n",
    "\n",
    "about 50 iterations is sufficient to estimate limiting solution\n",
    "\n",
    "Google is computing this pagerank everyday over the entire web-graph(10 billion+ nodes)\n",
    "\n",
    "1. Personalized Pagerank (PPR)\n",
    "    \n",
    "    Only difference is that instead of teleporting to any node in a graph we only teleport to a subset of the nodes in the graph\n",
    "    \n",
    "2. Random walk with restarts\n",
    "    \n",
    "    Teleport always to the starting node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3cff0d-49bd-4c9b-8e52-210569c271f2",
   "metadata": {},
   "source": [
    "### Introduction to graph neural Networks\n",
    "\n",
    "Formulating the task as an optimization problem\n",
    "\n",
    "$min_\\theta L(y, f(x))$ ← is the objective function\n",
    "\n",
    "here $\\theta$ is the set of features we optimize\n",
    "\n",
    "$\\theta$ = {Z} could contain one or more scalars, vectors or matrices\n",
    "\n",
    "One common loss for classification is the cross entropy (CE) \n",
    "\n",
    "How to optimize the objective function\n",
    "\n",
    "Gradient Descent\n",
    "\n",
    "Stochastic gradient descent\n",
    "\n",
    "Mini-batch gradient descent\n",
    "\n",
    "### A GNN layer\n",
    "\n",
    "gnn layer = message + aggregation\n",
    "\n",
    "1. How we define a gnn layer?\n",
    "2. How we stack those gnn layer?\n",
    "3. How we create a computation graph?\n",
    "\n",
    "**A Single Layer of GNN**\n",
    "\n",
    "message + aggregation → GNN layer\n",
    "\n",
    "{ GCN, GraphSage, GAT → different gnn architectures}\n",
    "\n",
    "<div>\n",
    "  <img src=\"Images/messagePassingGNN.png\" alt=\"messagePassing\" style=\"width: 500px; height: 300px;\">\n",
    "</div>\n",
    "\n",
    "1. Message Computation\n",
    "    \n",
    "    $m^{(l)} = MSG^{(l)}(h_u^{l-1})$\n",
    "    \n",
    "    $m^{(l)} = W^{(l)}(h_u^{l-1})$\n",
    "    \n",
    "    Multiply node features with Weight matrix $W^{(l)}$\n",
    "    \n",
    "2. Aggregation\n",
    "    \n",
    "    $h_v^{(l)} = AGG^{(l)}(\\{ m_u^{(l)}, u \\in N(v) \\})$\n",
    "    \n",
    "    AGG function could be anything eg:- sum(), mean() or max() aggregator\n",
    "    \n",
    "    $h_v^{(l)} = SUM^{(l)}(\\{ m_u^{(l)}, u \\in N(v) \\})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49e63ce-94d2-4e73-bd53-f34c0bb20822",
   "metadata": {},
   "source": [
    "issue → Information from node itself gets lost\n",
    "\n",
    "solution → Include prev layer embedding of node v when computing for new layer\n",
    "\n",
    "So it becomes\n",
    "\n",
    "1. Message Computation\n",
    "    \n",
    "    $m_u^{(l)} = W^{(l)}(h_u^{l-1})$                    For neighborhood nodes u \n",
    "    \n",
    "    $m_v^{(l)} = B^{(l)}(h_v^{l-1})$                      For node v itself\n",
    "    \n",
    "2. Aggregation\n",
    "    \n",
    "    $h_v^{(l)} = CONCAT(AGG^{(l)}(\\{ m_u^{(l)}, u \\in N(v) \\}), m_v^{(l)})$\n",
    "    \n",
    "    you can use concatenation or summation\n",
    "    \n",
    "3. Non-linear activation\n",
    "    \n",
    "    Adds expressiveness\n",
    "    \n",
    "    $\\sigma(.), Relu(.) , etc$\n",
    "    \n",
    "\n",
    "These three steps become a single layer of GNN\n",
    "\n",
    "### Classical GNN layer types\n",
    "\n",
    "1. **Graph Convolutional Network (GCN)**\n",
    "    \n",
    "    $x_i^{(k)} = \\sum_{j \\in N(i) \\cup \\{i\\}} \\frac{1}{\\sqrt{deg(i)}\\sqrt{deg(j)}}(W^T.x_j^{(k-1)}+b)$\n",
    "    \n",
    "    1. Message computation\n",
    "        \n",
    "        $m_j^{(k)} = \\frac{1}{\\sqrt{deg(i)}\\sqrt{deg(j)}} W^{T}(x_j^{k-1})$                                                    ← Normalized the degree of the node\n",
    "        \n",
    "    2. Aggregation\n",
    "        \n",
    "        $x_i^{(k)} = \\sigma(sum(\\{ m_j^{(k)}, j \\in N(i) \\cup{i} \\}))$\n",
    "        \n",
    "    \n",
    "    In matrix form\n",
    "    \n",
    "    1. $X^{(k+1)} = \\sigma(D^{-1}AX^{(k)}W^{(k)})$\n",
    "    2. $X^{(l+1)} = \\sigma(\\tilde{D}^{-\\frac{1}{2}} \\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)})$\n",
    "    \n",
    "    D is degree matrix (diagonal matrix)\n",
    "    \n",
    "    A is adjacency matrix\n",
    "    \n",
    "    X is feature or embedding matrix\n",
    "    \n",
    "    W is weight matrix\n",
    "    \n",
    "    D inverse is calculate to ensure normalization so that nodes with higher degrees do not disproportionately affect the aggregation or propagation process\n",
    "    \n",
    "    Both formulas are the same only in second formula the degree matrix is normalized on both size of adjacency matrix to ensure numerical stability\n",
    "    \n",
    "2. **SageConv**\n",
    "    \n",
    "    $x_i' = W_1x_i + W_2.mean_{j \\in N(i)}x_j$\n",
    "    \n",
    "    if project = True\n",
    "    \n",
    "    then we first compute $x_j \\leftarrow \\sigma(W_3x_j + b)$\n",
    "    \n",
    "3. **GraphSage**\n",
    "    \n",
    "    $h_v^{(l)} = \\sigma(W^{(l)}. CONCAT(h_v^{l-1},AGG(\\{ h_u^{(l-1)}, \\forall u \\in N(v)  \\})))$\n",
    "    \n",
    "    aggregation of a node’s self embedding along with neighbors\n",
    "    \n",
    "    1. Message computation\n",
    "        \n",
    "        $AGG(\\{ h_u^{(l-1)}, \\forall u \\in N(v)  \\})$\n",
    "        \n",
    "    2. Aggregation\n",
    "        1. Stage 1\n",
    "            \n",
    "            $h_{N(v)}^{(l)} \\leftarrow AGG(\\{ h_u^{(l-1)}, \\forall u \\in N(v)  \\})$\n",
    "            \n",
    "        2. Stage 2\n",
    "            \n",
    "            $h_v^{(l)} \\leftarrow \\sigma(W^{(l)}. CONCAT(h_v^{l-1}, h_{N(v)}^{(l)}))$\n",
    "            \n",
    "    \n",
    "    Aggregation functions\n",
    "    \n",
    "    1. Mean\n",
    "        \n",
    "        $AGG = \\sum_{u \\in N(v)} \\frac{h_u^{(l-1)}}{|N(v)|}$     \n",
    "        \n",
    "    2. Pool (mean or max any pooling can be used)\n",
    "        \n",
    "        $AGG = Mean(\\{ MLP(h_u^{(l-1)}), \\forall u \\in N(v) \\})$\n",
    "        \n",
    "    3. LSTM\n",
    "        \n",
    "        $AGG = LSTM([h_u^{(l-1)},\\forall u \\in \\pi (N(v))])$\n",
    "        \n",
    "        here LSTM is a sequence model that ignores the order of the nodes\n",
    "        \n",
    "    \n",
    "    As embedding vectors have different scales\n",
    "    After applying L2 normalization all vectors will have same scales\n",
    "    \n",
    "4. **The Label Propagation Algorithm (LPA)**\n",
    "    \n",
    "    $Y^{(k+1)} = D^{-1}AY^{(k)}$                       \n",
    "    Here all the nodes propagate label to their neighbors\n",
    "    \n",
    "    $y_i^{(k+1)} = y_i^{(0)}, \\forall i \\le m$                 \n",
    "    \n",
    "    Here all labeled nodes are reset to their initial values as we want to persist the labels of already labelled nodes\n",
    "    \n",
    "5. **GCN-LPA**\n",
    "    \n",
    "    increasing the strength of edges between the nodes of the same class is equivalent to increasing the accuracy of LPA’s predictions\n",
    "    \n",
    "    We want to find optimal node embeddings W* and edge weights A* by setting\n",
    "    \n",
    "    $W^* A^* = arg min_{W,A} L_{gcn}(W,A) + \\lambda L_{gcn} (A)$\n",
    "    \n",
    "6. **GAT**\n",
    "    \n",
    "    $h_v^{(l)} = \\sigma(\\sum_{u \\in N(v) }\\alpha_{vu} W^{(l)}h_{(u)}^{(l-1)})$\n",
    "    \n",
    "    here $\\alpha_{vu}$ are the attention weights\n",
    "    \n",
    "    In GCN/GraphSage $\\alpha_{vu}$ =$\\frac{1}{|N(v)|}$  making all neighbors equally important\n",
    "    \n",
    "    $\\alpha_{vu}$ is the importance of information coming for node u for the node v → giving each neighbor different neighbors\n",
    "    \n",
    "    $\\alpha_{vu}$ focuses on the important part of the data and ignores the rest\n",
    "    \n",
    "    for attention we compute attention coefficients $e_{vu}$\n",
    "    \n",
    "    $e_{vu} = a(W^{(l)}h_{u}^{(l-1)},W^{(l)}h_{v}^{(l-1)})$\n",
    "    \n",
    "    $e_{vu}$ indicates the importance of u’s mesage to node v\n",
    "    \n",
    "    Normalize $e_{vu}$ into final attention weights $\\alpha_{vu}$\n",
    "    \n",
    "    Using the softmax function so that $\\sum_{u\\in N(v)} \\alpha_{vu} = 1$\n",
    "    \n",
    "    $\\alpha_{vu} = \\frac{\\exp(e_{vu})}{\\sum_{k\\in N(v)}\\exp(e_{vk})}$\n",
    "    \n",
    "    Weighted sum based on final attention weight $\\alpha_{vu}$\n",
    "    \n",
    "    $h_v^{(l)} = \\sigma(\\sum_{u \\in N(v) }\\alpha_{vu} W^{(l)}h_{(u)}^{(l-1)})$\n",
    "    \n",
    "    What is the form of the attention mechanism (a)\n",
    "    \n",
    "    $e_{vu} = a(W^{(l)}h_{u}^{(l-1)},W^{(l)}h_{v}^{(l-1)})$\n",
    "    \n",
    "    here a could be a single-layer neural network\n",
    "    \n",
    "    $e_{vu} = Linear(Concat(W^{(l)}h_{u}^{(l-1)},W^{(l)}h_{v}^{(l-1)}))$\n",
    "    \n",
    "    Multi-headed attention\n",
    "    \n",
    "    stabilizes the learning process of attention mechanism\n",
    "    \n",
    "    create multiple attention scores[each replica has different set of parameters]:\n",
    "    \n",
    "    $h_v^{(l)}[1] = \\sigma(\\sum_{u \\in N(v) }\\alpha_{vu}^{1} W^{(l)}h_{(u)}^{(l-1)})$\n",
    "    \n",
    "    $h_v^{(l)}[2] = \\sigma(\\sum_{u \\in N(v) }\\alpha_{vu}^{2} W^{(l)}h_{(u)}^{(l-1)})$\n",
    "    \n",
    "    $h_v^{(l)}[3] = \\sigma(\\sum_{u \\in N(v) }\\alpha_{vu}^{3} W^{(l)}h_{(u)}^{(l-1)})$\n",
    "    \n",
    "    Outputs are aggregated\n",
    "    \n",
    "    $h_v^{(l)} = AGG(h_v^{(l)}[1],h_v^{(2)}[2],h_v^{(3)}[3])$\n",
    "    \n",
    "    AGG could be concatenation or summation\n",
    "    \n",
    "7. **Edge Convolutional Layer**\n",
    "    \n",
    "    $x_i^{(k)} = \\max\\limits_{j\\in N(i)}h_\\theta(x_i^{(k-1)},x_j^{(k-1)} -x_i^{(k-1)})$\n",
    "    \n",
    "    $h_\\theta$ denotes an MLP\n",
    "    \n",
    "\n",
    "General GNN layer \n",
    "\n",
    "- Linear\n",
    "- BatchNorm\n",
    "- Dropout\n",
    "- Activation\n",
    "- Attention\n",
    "- Aggregation\n",
    "\n",
    "### Things to work on graphs\n",
    "\n",
    "Class imbalance → focal loss\n",
    "\n",
    "Dataloader\n",
    "\n",
    "Data transformation\n",
    "\n",
    "Models\n",
    "\n",
    "GCN\n",
    "\n",
    "GraphSage\n",
    "\n",
    "GAT\n",
    "\n",
    "GATv2Conv\n",
    "\n",
    "GCN-LPA\n",
    "\n",
    "Visualisation?\n",
    "\n",
    "Types of graphs → directed, weighted?\n",
    "\n",
    "Why and how to use graph models\n",
    "\n",
    "- **Integration**: Node2Vec embeddings can serve as initial node representations fed into GNNs, enhancing the GNN's ability to generalize and capture nuanced graph structures.\n",
    "- **Enhancement**: GNNs can further refine node embeddings learned by Node2Vec through iterative message passing and feature aggregation, improving their robustness and discriminative power for specific tasks.\n",
    "\n",
    "List of Graph models\n",
    "\n",
    "1. Embedding based models\n",
    "    1. DeepWalk\n",
    "    2. node2vec\n",
    "    3. LINE\n",
    "2. Convolution graph models\n",
    "    1. GCN\n",
    "    2. GraphSAGE\n",
    "    3. GAT\n",
    "    4. ChebNet\n",
    "    5. MixHop\n",
    "    6. Graph Isomopphism Network (GIN)\n",
    "    7. Graph Convolution Matrix Completion (GCMC)\n",
    "3. Recurrent and other model\n",
    "    1. Gated Graph Neural Networks (GGNN)\n",
    "    2. Graph Recurrent Neural Network (GRNN)\n",
    "    3. Graph U-Net\n",
    "4. Graph Generative Models\n",
    "    1. GraphRNN\n",
    "    2. GraphVAE\n",
    "5. Graph Reinforcement Learning Models\n",
    "    1. DQN-GNN\n",
    "    2. Graph Actor-Critic\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeb0880-8cf9-4691-984a-89719ccab75f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cuda118",
   "language": "python",
   "name": "cuda118"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
