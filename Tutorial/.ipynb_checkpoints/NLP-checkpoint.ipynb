{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0253ef37-0f95-4789-a3cd-a4cbdcc1b92e",
   "metadata": {},
   "source": [
    "## Natural Language Processing with RNNs and Attention\n",
    "\n",
    "### word2vec\n",
    "\n",
    "CBOW  → given neighbor nodes predict the current word → like fill in the blanks\n",
    "\n",
    "Skip-gram → given a word predict surrounding words or next words\n",
    "\n",
    "Training word2vec\n",
    "\n",
    "<div>\n",
    "  <img src=\"Images/word2vec.png\" alt=\"word2vec\" style=\"width: 500px; height: 300px;\">\n",
    "</div>\n",
    "\n",
    "1. Input layer: Takes in the one-hot encoded vector of a word [ from the whole vocabulary] - length is same as the number of words in vocab\n",
    "2. Hidden layer: The weights of this layer become the word embeddings → 300 neurons \n",
    "3. Output layer: Used to predict the context words or target word depending on the training approach (CBOW or skip-gram)\n",
    "\n",
    "After training the word2vec model the hidden layer weights become the word embedding \n",
    "\n",
    "The rest of the model can be discarded only the middle layer is the most important and a pretrained model can used for getting node embeddings\n",
    "\n",
    "## GloVe\n",
    "\n",
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c502943a-5bd0-4c0e-a94b-ef6bb3bad632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EnvCuda118",
   "language": "python",
   "name": "envcuda118"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
